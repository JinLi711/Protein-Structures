{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import importlib\n",
    "# importlib.reload(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../data/cull%i/model_data/\" % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devtest_aa_dict = np.load(data_path + 'devtest_aa_dict.npy')[()]\n",
    "devtest_cmap_dict = np.load(data_path + 'devtest_cmap_dict.npy')[()]\n",
    "train_aa_dict = np.load(data_path + 'train_aa_dict.npy')[()]\n",
    "train_cmap_dict = np.load(data_path + 'train_cmap_dict.npy')[()]\n",
    "valid_aa_dict = np.load(data_path + 'valid_aa_dict.npy')[()]\n",
    "valid_cmap_dict = np.load(data_path + 'valid_cmap_dict.npy')[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# devtest_aa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# from keras.engine.topology import Layer\n",
    "# # from keras.layers import Layer\n",
    "\n",
    "# # Need to create my own outer product layer\n",
    "# class OuterConcat(Layer):\n",
    "#     def __init__(self, output_dim, **kwargs):\n",
    "#         self.output_dim = output_dim\n",
    "#         self.name = \"HII\"\n",
    "        \n",
    "#     def built(self, input_shape):\n",
    "#         # Create a trainable weight variable for this layer.\n",
    "#         self.kernel = self.add_weight(name='kernel', \n",
    "#                                       shape=(input_shape[1], self.output_dim),\n",
    "#                                       initializer='uniform',\n",
    "#                                       trainable=True)\n",
    "#         super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         return K.dot(x, self.kernel)\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seq2pairwise(x)\n",
    "# x = [1, 2, 3]\n",
    "# y = [4, 5, 6]\n",
    "# X, Y = tf.meshgrid(x, y)\n",
    "# X.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def seq2pairwise(incoming):\n",
    "#     L = tf.shape(incoming)[1]\n",
    "#     #save the indexes of each position\n",
    "#     v = tf.range(0, L, 1)\n",
    "#     i, j = tf.meshgrid(v, v)\n",
    "#     m = (i+j)/2\n",
    "#     #switch batch dim with L dim to put L at first\n",
    "#     incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "#     #full matrix i with element in incomming2 indexed i[i][j]\n",
    "#     out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "#     out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "#     out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "#     #concatante final feature dim together\n",
    "#     out = tf.concat([out1, out2, out3], axis=3)\n",
    "#     #return to original dims\n",
    "#     output = tf.transpose(out, perm=[2, 0, 1, 3])\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyDenseLayer object at 0x182aa91208>\n",
      "Tensor(\"my_dense_layer/MatMul:0\", shape=(10, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# class MyDenseLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_outputs):\n",
    "#         super(MyDenseLayer, self).__init__()\n",
    "#         self.num_outputs = num_outputs\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.kernel = self.add_variable(\n",
    "#             \"kernel\",\n",
    "#             shape=[int(input_shape[-1]),\n",
    "#                    self.num_outputs])\n",
    "\n",
    "#     def call(self, input):\n",
    "#         return tf.matmul(input, self.kernel)\n",
    "\n",
    "\n",
    "# layer = MyDenseLayer(10)\n",
    "# print(layer)\n",
    "# print(layer(tf.zeros([10, 5])))\n",
    "# print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OuterProduct(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Given a layer of size (B, L, N), create \n",
    "    a layer of size (B, L, L, 3N).\n",
    "    For example, if we have \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(OuterProduct, self).__init__()\n",
    "\n",
    "    def call(self, incoming):\n",
    "\n",
    "        L = tf.shape(incoming)[1]\n",
    "        # save the indexes of each position\n",
    "        v = tf.range(0, L, 1)\n",
    "\n",
    "        i, j = tf.meshgrid(v, v)\n",
    "\n",
    "        m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "        # switch batch dim with L dim to put L at first\n",
    "        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "        # full matrix i with element in incomming2 indexed i[i][j]\n",
    "        out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "        out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "        out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "        # concatanate final feature dim together\n",
    "        out = tf.concat([out1, out2, out3], axis=3)\n",
    "        # return to original dims\n",
    "        output = tf.transpose(out, perm=[2, 0, 1, 3])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock1D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    All the layers in the residual block will have the \n",
    "    same number of features and stride.\n",
    "    There are two layers in this block.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stride, activation):\n",
    "        self.stride = stride\n",
    "        self.activation = activation\n",
    "        super(ResidualBlock1D, self).__init__()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         # Create a trainable weight variable for this layer.\n",
    "#         self.kernel = self.add_weight(\n",
    "#             name='kernel',\n",
    "#             shape=(input_shape[1], self.output_dim),\n",
    "#             initializer='uniform',\n",
    "#             trainable=True\n",
    "#         )\n",
    "#         # Be sure to call this at the end\n",
    "#         super(MyLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        size = int(x.shape[-1])\n",
    "        y = tf.keras.layers.Conv1D(\n",
    "            size,\n",
    "            self.stride,\n",
    "            activation=self.activation,\n",
    "        )(x)\n",
    "        y = tf.keras.layers.Conv1D(\n",
    "            size,\n",
    "            self.stride,\n",
    "            activation=self.activation,\n",
    "        )(y)\n",
    "\n",
    "        y = tf.keras.layers.add([y, x])\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class ResidualBlock2D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    All the layers in the residual block will have the \n",
    "    same number of features and stride.\n",
    "    There are two layers in this block.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stride, activation):\n",
    "        self.stride = stride\n",
    "        self.activation = activation\n",
    "        super(ResidualBlock2D, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        size = int(x.shape[-1])\n",
    "        y = tf.keras.layers.Conv2D(\n",
    "            size,\n",
    "            self.stride,\n",
    "            activation=self.activation,\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        y = tf.keras.layers.Conv2D(\n",
    "            size,\n",
    "            self.stride,\n",
    "            activation=self.activation,\n",
    "            padding='same'\n",
    "        )(y)\n",
    "\n",
    "        y = tf.keras.layers.add([y, x])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def conv_block(x, size, stride, conv_dim, num_layers, activation, padding):\n",
    "#     if conv_dim == 1:\n",
    "#         for i in range(num_layers):\n",
    "#             x = tf.keras.layers.Conv1D(\n",
    "#                 size,\n",
    "#                 stride,\n",
    "#                 activation=activation,\n",
    "#                 padding=padding\n",
    "#             )(x)\n",
    "#             x = tf.keras.layers.BatchNormalization()(x)\n",
    "#         return x\n",
    "\n",
    "#     elif conv_dim == 2:\n",
    "#         for i in range(num_layers):\n",
    "#             x = tf.keras.layers.Conv2D(\n",
    "#                 size,\n",
    "#                 stride,\n",
    "#                 activation=activation,\n",
    "#                 padding=padding\n",
    "#             )(x)\n",
    "#             x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "#         return x\n",
    "#     else:\n",
    "#         raise ValueError(\"Not an available convnet dimension\")\n",
    "\n",
    "\n",
    "def residual_conv_block(x, convnet, stride, num_layers=2, activation=\"relu\", padding=\"same\"):\n",
    "\n",
    "    size = int(x.shape[-1])\n",
    "    y = x\n",
    "    \n",
    "    if convnet == \"1d convnet\":\n",
    "        for i in range(num_layers):\n",
    "            y = tf.keras.layers.Conv1D(\n",
    "                size,\n",
    "                stride,\n",
    "                activation=activation,\n",
    "                padding=padding\n",
    "            )(y)\n",
    "            y = tf.keras.layers.BatchNormalization()(y)\n",
    "    elif convnet == \"2d convnet\":\n",
    "        for i in range(num_layers):\n",
    "            y = tf.keras.layers.Conv2D(\n",
    "                size,\n",
    "                stride,\n",
    "                activation=activation,\n",
    "                padding=padding\n",
    "            )(y)\n",
    "            y = tf.keras.layers.BatchNormalization()(y)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Not an available convnet dimension\")\n",
    "        \n",
    "    y = tf.keras.layers.add([y, x])\n",
    "    return y\n",
    "\n",
    "\n",
    "# def Residual_block_2d(x, stride, num_layers=2, activation='relu', padding='same'):\n",
    "\n",
    "#     size = int(x.shape[-1])\n",
    "\n",
    "# #     y = conv_block(x, size, stride, conv)\n",
    "\n",
    "#     y = conv_block(x, size, stride, 2, num_layers, activation, padding)\n",
    "#     y = tf.keras.layers.add([y, x])\n",
    "\n",
    "#     return y\n",
    "\n",
    "\n",
    "# def Residual_block_1d(x, stride, num_layers=2, activation='relu', padding='same'):\n",
    "\n",
    "#     size = int(x.shape[-1])\n",
    "\n",
    "# #     y = conv_block(x, size, stride, conv)\n",
    "\n",
    "#     y = conv_block(x, size, stride, 1, num_layers, activation, padding)\n",
    "#     y = tf.keras.layers.add([y, x])\n",
    "\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.keras.layers.Conv1D()\n",
    "x = 1\n",
    "y = x\n",
    "\n",
    "y = y + 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, None, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, None, 40)     840         input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, None, 60)     2460        conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, None, 60)     3660        conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, 60)     240         conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, None, 60)     3660        batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, 60)     240         conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, None, 60)     0           batch_normalization_59[0][0]     \n",
      "                                                                 conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "outer_product_18 (OuterProduct) (None, None, None, 1 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 1 32580       outer_product_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 1 720         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 1 32580       batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 1 720         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
      "                                                                 outer_product_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 2 362         add_35[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 78,062\n",
      "Trainable params: 77,102\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# from keras import Input\n",
    "\n",
    "# from keras.models import Model\n",
    "# from tensorflow.keras.model import Model\n",
    "# from keras import layers\n",
    "# from tf.keras.layers\n",
    "def create_architecture():\n",
    "    input_tensor = tf.keras.Input(shape=(None, 20))\n",
    "    x = tf.keras.layers.Conv1D(\n",
    "        40,\n",
    "        1,\n",
    "        activation='relu',\n",
    "    )(input_tensor)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(\n",
    "        60,\n",
    "        1,\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "\n",
    "#     x = ResidualBlock1D(1, \"relu\")(x)\n",
    "\n",
    "\n",
    "#     x = Residual_block_1d(x, 1)\n",
    "#     residual_conv_block(x, convnet, stride, num_layers=2, activation=\"relu\", padding=\"same\")\n",
    "    x = residual_conv_block(x, \"1d convnet\", 1)\n",
    "\n",
    "    \"\"\"\n",
    "    EDIT: did not have to do this lol.\n",
    "    Keras 2.2.4\n",
    "\n",
    "    To make this work, I had to go to:\n",
    "    /Users/jinli/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\n",
    "\n",
    "    and change on line: 1749\n",
    "    layer.outbound_nodes.append(self)\n",
    "\n",
    "    to\n",
    "\n",
    "    layer._outbound_nodes.append(self)\n",
    "    \"\"\"\n",
    "\n",
    "    x = OuterProduct()(x)\n",
    "#     x = ResidualBlock2D(1, \"relu\")(x)\n",
    "#     x = Residual_block_2d(x, 1)\n",
    "    # x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = residual_conv_block(x, \"2d convnet\", 1)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(2, 1, activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.Flatten()(x)\n",
    "    model = tf.keras.models.Model(\n",
    "        input_tensor,\n",
    "        x\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# x = MyDenseLayer(10)(x)\n",
    "\n",
    "# third column is the number of feature maps\n",
    "model = create_architecture()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def aa_generator(x, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = x.copy()\n",
    "    outputs = y.copy()\n",
    "    keys = set (x.keys())\n",
    "#     print(keys)\n",
    "    while True:\n",
    "        try:\n",
    "            key = random.sample(keys, 1)[0]\n",
    "            keys.remove(key)\n",
    "            \n",
    "            one_hot_aa = x[key]\n",
    "            one_hot_aa = np.reshape(one_hot_aa, (1,) + one_hot_aa.shape)\n",
    "            cmap = y[key]\n",
    "            cmap = np.reshape(cmap, (1,) + cmap.shape + (1,))\n",
    "            yield one_hot_aa, cmap\n",
    "\n",
    "#             yield key\n",
    "        except ValueError:\n",
    "            # if out of keys, reinsert back the keys\n",
    "            inputs = x.copy()\n",
    "            outputs = y.copy()\n",
    "            keys = set (x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinli/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 15s 2s/step - loss: 0.7847\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x182e044b70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    sample_weight_mode=\"temporal\"\n",
    ")\n",
    "model.fit_generator(\n",
    "    aa_generator(train_aa_dict, train_cmap_dict),\n",
    "    steps_per_epoch=10, \n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = OuterProduct()\n",
    "test1 = test(tf.zeros([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf.zeros([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# int (x.shape[-1])\n",
    "\n",
    "# output and input from preprocessing probably doesn't align\n",
    "import math\n",
    "math.sqrt(24649)\n",
    "math.sqrt(24336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test1.shape\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeros = tf.zeros([10, 5, 6])\n",
    "type(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model1 = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(10, activation='softmax'),\n",
    "# #     MyDenseLayer(10),\n",
    "# #     test,\n",
    "# ])\n",
    "\n",
    "# # model2 = tf.keras.models.Sequential([\n",
    "# #     test()\n",
    "# # ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model2.summary()\n",
    "model1.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 + 3 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import models\n",
    "# from keras import layers\n",
    "# network = tf.keras.models.Sequential()\n",
    "# network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "# network.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# network.compile(optimizer='rmsprop',\n",
    "#                 loss='categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     # Adds a densely-connected layer with 64 units to the model:\n",
    "#     layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "#     # Add another:\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     # Add a softmax layer with 10 output units:\n",
    "#     layers.Dense(10, activation='softmax')])\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.train.AdamOptimizer(0.001),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "keys = []\n",
    "for item in aa_generator(train_aa_dict, train_cmap_dict):\n",
    "    if i == 142:\n",
    "        break\n",
    "    keys.append(item)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_aa_dict['1']\n",
    "# keys\n",
    "# len (aa_generator)\n",
    "len (set (keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len (train_aa_dict.keys())\n",
    "array = train_aa_dict['16vp']#.reshape(-1)\n",
    "np.reshape(array, (1,) + array.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet_identity_block_8_1/Relu_2:0\", shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block_8/conv2d_39/kernel:0', 'resnet_identity_block_8/conv2d_39/bias:0', 'resnet_identity_block_8/batch_normalization_24/gamma:0', 'resnet_identity_block_8/batch_normalization_24/beta:0', 'resnet_identity_block_8/conv2d_40/kernel:0', 'resnet_identity_block_8/conv2d_40/bias:0', 'resnet_identity_block_8/batch_normalization_25/gamma:0', 'resnet_identity_block_8/batch_normalization_25/beta:0', 'resnet_identity_block_8/conv2d_41/kernel:0', 'resnet_identity_block_8/conv2d_41/bias:0', 'resnet_identity_block_8/batch_normalization_26/gamma:0', 'resnet_identity_block_8/batch_normalization_26/beta:0']\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 2, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_8 (Res (None, 2, 3, 3)           41        \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 29\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__(name='')\n",
    "        filters1, filters2, filters3 = filters\n",
    "\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2b = tf.keras.layers.Conv2D(\n",
    "            filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "\n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_tensor = tf.keras.Input(shape=(2, 3, 3))\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "nextz = block(input_tensor)\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    input_tensor,\n",
    "    nextz\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
