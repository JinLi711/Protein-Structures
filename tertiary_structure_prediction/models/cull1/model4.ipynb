{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the network used in:\n",
    "\n",
    "*Wang S, Sun S, Li Z, Zhang R, Xu J (2017) \n",
    "Accurate De Novo Prediction of Protein Contact Map by \n",
    "Ultra-Deep Learning Model. PLoS Comput Biol 13(1): e1005324. \n",
    "https://doi.org/10.1371/journal.pcbi.1005324*\n",
    "\n",
    "Here are some attributes they used in the paper:\n",
    "s\n",
    "Inputs:\n",
    "    Along with 1 hot encoding of amino acids to a 20 dimension\n",
    "    1 hot encoding, they also included another 6 dimensions,\n",
    "    3-state secondary structure and 3-state solvent accessibility.\n",
    "    This was predicted using another neural network.\n",
    "    pairwise features: \n",
    "        mutual information, \n",
    "        the EC information calculated by CCMpred, \n",
    "        and pair- wise contact potential\n",
    "    were concatenated after the outer product layer\n",
    "\n",
    "Activation layer:\n",
    "    ReLU after every layer.\n",
    "    Batch normalization before activation layer.\n",
    "    (though did not say whether this was after \n",
    "    or before the convolution layer)\n",
    "\n",
    "Residual network:\n",
    "    the number of features of the next layer is greater or\n",
    "    equal to the one below it, so they had to pad\n",
    "    the previous layer with zeros to allow the skip adding.\n",
    "    For 1D residual network:\n",
    "        window size: 17 (fixed)\n",
    "        number of layers: 6 (fixed)\n",
    "    For 2D residual network:\n",
    "        window size: (3,3) or (5,5)\n",
    "        number of layers: ~60\n",
    "        number of hidden neurons per layer: ~60\n",
    "\n",
    "Loss function:\n",
    "    negative log-likelihood averaged over all \n",
    "    the residue pairs of the training proteins.\n",
    "    Since outputs were unbalanced, they assign a larger \n",
    "    weight to the residue pairs forming a contact.\n",
    "    The weight is assigned such that the total weight \n",
    "    assigned to contacts is approximately 1/8 of the number \n",
    "    of non-contacts in the training set.\n",
    "\n",
    "Mini-batches:\n",
    "    can have mini-batches, but they sorted the training set \n",
    "    and then grouped batches by related size. \n",
    "    Then they did some extra padding to make sure all proteins\n",
    "    in the batch had the same size.\n",
    "\n",
    "Others:\n",
    "    L2 normalization\n",
    "    stochastic gradient descent\n",
    "    drop out was never mentioned\n",
    "    20-30 epochs\n",
    "\n",
    "\n",
    "Things that I did not implement, even though I wish I did:\n",
    "    the extra six dimensions added to input\n",
    "    pairwise features\n",
    "    different layer sizes in residual network\n",
    "    used sparse categorical crossentropy instead of log\n",
    "    no weighing of outputs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Special Layers\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class OuterProduct(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Given a layer of size (B, L, N), create \n",
    "    a layer of size (B, L, L, 3N).\n",
    "    If we have {v1, ..., vm},\n",
    "    for the i, j entry, we have (vi, v((i+j)/2), vj).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterProduct, self).__init__()\n",
    "\n",
    "    def call(self, incoming):\n",
    "        \"\"\"\n",
    "        Create the layer.\n",
    "\n",
    "        :param incoming: tensor of size (B, L, N)\n",
    "        :type  incoming: tensorflow.python.framework.ops.Tensor\n",
    "        :returns: tensor of size (B, L, L, 3N)\n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        L = tf.shape(incoming)[1]\n",
    "        # save the indexes of each position\n",
    "        v = tf.range(0, L, 1)\n",
    "\n",
    "        i, j = tf.meshgrid(v, v)\n",
    "\n",
    "        m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "        # switch batch dim with L dim to put L at first\n",
    "        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "        # full matrix i with element in incomming2 indexed i[i][j]\n",
    "        out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "        out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "        out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "        # concatanate final feature dim together\n",
    "        out = tf.concat([out1, out2, out3], axis=3)\n",
    "        # return to original dims\n",
    "        output = tf.transpose(\n",
    "            out,\n",
    "            perm=[2, 0, 1, 3],\n",
    "            name=\"outer_product\"\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "def residual_conv_block(\n",
    "        x,\n",
    "        convnet,\n",
    "        stride,\n",
    "        num_layers,\n",
    "        regularizer=None,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"):\n",
    "    \"\"\"\n",
    "    Create a residual convolution block, either \n",
    "    in 1 or 2 dimensions.\n",
    "\n",
    "    :param x: \n",
    "    :type x:  \n",
    "    :param convnet: indicates which type of layer to use\n",
    "    :type  convnet: string\n",
    "    :param stride: stride\n",
    "    :type  stride: int\n",
    "    :param num_layers: number of layers for the entire residual network\n",
    "    :type  num_layers: int\n",
    "    :param regularizer: \n",
    "    :type  regularizer: \n",
    "    :param activation: \n",
    "    :type  activation: str\n",
    "    :param padding:\n",
    "    :type  padding: str\n",
    "    :returns: result of the residual network\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    size = int(x.shape[-1])\n",
    "    y = x\n",
    "\n",
    "    if num_layers % 2 != 0:\n",
    "        raise ValueError(\"The number of layers must be even\")\n",
    "\n",
    "    def one_dim_block(x, i):\n",
    "        \"\"\"\n",
    "        Create the duo layer for conv1d.\n",
    "\n",
    "        :param x: input\n",
    "        :type  x: tensorflow.python.framework.ops.Tensor\n",
    "        :param i: position of that duo layer\n",
    "        :type  i: int\n",
    "        :returns: \n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        i += 1\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}a\".format(i),\n",
    "        )(x)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}a\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}b\".format(i),\n",
    "        )(z)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}b\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.add(\n",
    "            [z, x],\n",
    "            name=convnet + \"_residual_block{}\".format(i)\n",
    "        )\n",
    "\n",
    "        return z\n",
    "\n",
    "    def two_dim_block(x, i):\n",
    "        \"\"\"\n",
    "        Create the duo layer for conv2d.\n",
    "\n",
    "        :param x: input\n",
    "        :type  x: tensorflow.python.framework.ops.Tensor\n",
    "        :param i: position of that duo layer\n",
    "        :type  i: int\n",
    "        :returns: \n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        i += 1\n",
    "        z = tf.keras.layers.Conv2D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}a\".format(i),\n",
    "        )(x)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}a\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.Conv2D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}b\".format(i),\n",
    "        )(z)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}b\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.add(\n",
    "            [z, x],\n",
    "            name=convnet + \"_residual_block{}\".format(i)\n",
    "        )\n",
    "\n",
    "        return z\n",
    "\n",
    "    if convnet == \"1d_convnet\":\n",
    "        for i in range(int(num_layers / 2)):\n",
    "            y = one_dim_block(y, i)\n",
    "\n",
    "    elif convnet == \"2d_convnet\":\n",
    "        for i in range(int(num_layers / 2)):\n",
    "            y = two_dim_block(y, i)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Not an available convnet dimension\")\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def inception_module (x):\n",
    "    \"\"\"\n",
    "    Create the inception V3 module\n",
    "    \n",
    "    :param x: Tensor input continuing from the chain\n",
    "    :type  x: tensorflow.python.framework.ops.Tensor\n",
    "    \n",
    "    :return: The concatenated output of the four branches\n",
    "    :rtype:  tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # from tensorflow.keras import layers\n",
    "\n",
    "    branch_a = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        1,\n",
    "        activation='relu',\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "\n",
    "    branch_b = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        1,\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    branch_b = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(branch_b)\n",
    "\n",
    "    branch_c = tf.keras.layers.AveragePooling2D(\n",
    "        3,\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    branch_c = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(branch_c)\n",
    "\n",
    "    branch_d = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        1,\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    branch_d = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(branch_d)\n",
    "    branch_d = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(branch_d)\n",
    "\n",
    "    output = tf.keras.layers.concatenate(\n",
    "        [branch_a, branch_b, branch_c, branch_d],\n",
    "        name=\"Inception_V3\"\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Generator\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def aa_generator(x, y):\n",
    "    \"\"\"\n",
    "    Generator for feeding a single instance of an \n",
    "    input and an output.\n",
    "    The generator is reset when all elements are used.\n",
    "\n",
    "    :param: input\n",
    "    :type:  dict\n",
    "    :param: label\n",
    "    :type:  dict\n",
    "    :returns: a single instance of input and label\n",
    "    :rtype:   (numpy array, numpy array)\n",
    "    \"\"\"\n",
    "\n",
    "    keys = set(x.keys())\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            key = random.sample(keys, 1)[0]\n",
    "            keys.remove(key)\n",
    "\n",
    "            one_hot_aa = x[key]\n",
    "            one_hot_aa = np.reshape(\n",
    "                one_hot_aa, (1,) + one_hot_aa.shape\n",
    "            )\n",
    "            cmap = y[key]\n",
    "            cmap = np.reshape(cmap, (1,) + cmap.shape + (1,))\n",
    "            yield one_hot_aa, cmap\n",
    "\n",
    "        except ValueError:\n",
    "            # if out of keys, reinsert back the keys\n",
    "            keys = set(x.keys())\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Create the model\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_architecture(\n",
    "    resid_layer2_window_size, \n",
    "    resid_layer2_num_layers):\n",
    "    \"\"\"\n",
    "    Create the basic architecture. \n",
    "    1d residual network followed by 2d residual network.\n",
    "\n",
    "    :param resid_layer2_window_size: window size\n",
    "    :type  resid_layer2_window_size: int\n",
    "    :param resid_layer2_num_layers: number of layers\n",
    "    :type  resid_layer2_num_layers: int\n",
    "    :returns: training model\n",
    "    :rtype:   tensorflow.python.keras.engine.training.Model\n",
    "    \"\"\"\n",
    "\n",
    "    input_tensor = tf.keras.Input(\n",
    "        shape=(None, 20),\n",
    "        name=\"input_layer\"\n",
    "    )\n",
    "\n",
    "    x = residual_conv_block(\n",
    "        input_tensor,\n",
    "        \"1d_convnet\",\n",
    "        17,\n",
    "        num_layers=6,\n",
    "        regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )\n",
    "\n",
    "    x = OuterProduct(\n",
    "    )(x)\n",
    "\n",
    "    x = residual_conv_block(\n",
    "        x,\n",
    "        \"2d_convnet\",\n",
    "        resid_layer2_window_size,\n",
    "        num_layers=resid_layer2_num_layers,\n",
    "        regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        2,\n",
    "        1,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(x)\n",
    "\n",
    "#     x = tf.keras.layers.Dropout(\n",
    "#         0.5,\n",
    "#         name=\"Drop-Out\"\n",
    "#     )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        input_tensor,\n",
    "        x\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Callbacks\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "weight_path = \"best_weight/{}_weights.best.hdf5\".format(\n",
    "    'Double_Resid_Network'\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    weight_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# tensorboard = tf.keras.callback.TensorBoard(\n",
    "#     log_dir='Logs',\n",
    "# )\n",
    "\n",
    "reduceLROnPlat = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    \"\"\"\n",
    "    Reduce learning rate after epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    import math \n",
    "\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(\n",
    "        drop, math.floor((1+epoch)/epochs_drop)\n",
    "    )\n",
    "    return lrate\n",
    "\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    # monitor='acc'\n",
    "    mode=\"min\",\n",
    "    verbose=2,\n",
    "    # training is interrupted when the monitor argument \n",
    "    # stops improving after n steps\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     import sys\n",
    "\n",
    "#     data_path = \"../../data/cull%i/model_data/\" % int (sys.argv[1])\n",
    "\n",
    "#     train_aa_dict = np.load(data_path + 'train_aa_dict.npy')[()]\n",
    "#     train_cmap_dict = np.load(data_path + 'train_cmap_dict.npy')[()]\n",
    "#     valid_aa_dict = np.load(data_path + 'valid_aa_dict.npy')[()]\n",
    "#     valid_cmap_dict = np.load(data_path + 'valid_cmap_dict.npy')[()]\n",
    "#     devtest_aa_dict = np.load(data_path + 'devtest_aa_dict.npy')[()]\n",
    "#     devtest_cmap_dict = np.load(data_path + 'devtest_cmap_dict.npy')[()]\n",
    "\n",
    "\n",
    "#     model = create_architecture(3, 60)\n",
    "#     print (model.summary())\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=\"adam\",\n",
    "#         loss=\"sparse_categorical_crossentropy\",\n",
    "#         sample_weight_mode=\"temporal\",\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "\n",
    "#     history = model.fit_generator(\n",
    "#         aa_generator(train_aa_dict, train_cmap_dict),\n",
    "#         validation_data=aa_generator(valid_aa_dict, valid_cmap_dict),\n",
    "#         steps_per_epoch=len(train_aa_dict), \n",
    "#         epochs=20,\n",
    "#         validation_steps=10,\n",
    "#         callbacks=callbacks_list\n",
    "#     )\n",
    "\n",
    "#     model.save('my_model.h5') \n",
    "\n",
    "#     # to load back the model:\n",
    "#     # model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class OuterProduct(tf.keras.layers.Layer):\n",
    "#     \"\"\"\n",
    "#     Given a layer of size (B, L, N), create \n",
    "#     a layer of size (B, L, L, 3N).\n",
    "#     If we have {v1, ..., vm},\n",
    "#     for the i, j entry, we have (vi, v((i+j)/2), vj).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(OuterProduct, self).__init__()\n",
    "\n",
    "#     def call(self, incoming):\n",
    "#         \"\"\"\n",
    "#         Create the layer.\n",
    "\n",
    "#         :param incoming: tensor of size (B, L, N)\n",
    "#         :type  incoming: tensorflow.python.framework.ops.Tensor\n",
    "#         :returns: tensor of size (B, L, L, 3N)\n",
    "#         :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "#         \"\"\"\n",
    "\n",
    "#         L = tf.shape(incoming)[1]\n",
    "#         # save the indexes of each position\n",
    "#         v = tf.range(0, L, 1)\n",
    "\n",
    "#         i, j = tf.meshgrid(v, v)\n",
    "#         print(i.shape)\n",
    "\n",
    "#         m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "#         # switch batch dim with L dim to put L at first\n",
    "#         incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "        \n",
    "# #         incoming2 = tf.sparse.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "#         # full matrix i with element in incomming2 indexed i[i][j]\n",
    "# #         print(incoming2.op)\n",
    "#         out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "#         out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "#         out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "# #         out1 = tf.gather(incoming2, i)\n",
    "# #         out1 = tf.dynamic_partition(incoming2, i, 4)[0]\n",
    "# #         out2 = tf.dynamic_partition(incoming2, j, 4)[0]\n",
    "# #         out3 = tf.dynamic_partition(incoming2, m, 4)[0]\n",
    "# #         out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "# #         out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "#         # concatanate final feature dim together\n",
    "#         out = tf.concat([out1, out2, out3], axis=3)\n",
    "#         # return to original dims\n",
    "#         output = tf.transpose(\n",
    "#             out,\n",
    "#             perm=[2, 0, 1, 3],\n",
    "#             name=\"outer_product\"\n",
    "#         )\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_layer2_window_size = 3\n",
    "resid_layer2_num_layers = 2\n",
    "\n",
    "input_tensor = tf.keras.Input(\n",
    "    shape=(None, 20),\n",
    "    name=\"input_layer\"\n",
    ")\n",
    "\n",
    "print(input_tensor.op)\n",
    "x = residual_conv_block(\n",
    "    input_tensor,\n",
    "    \"1d_convnet\",\n",
    "    17,\n",
    "    num_layers=6,\n",
    "    regularizer=tf.keras.regularizers.l2(0.001)\n",
    ")\n",
    "\n",
    "print(\"INCOMING: \", x.op )\n",
    "x = OuterProduct(\n",
    ")(x)\n",
    "\n",
    "x = residual_conv_block(\n",
    "    x,\n",
    "    \"2d_convnet\",\n",
    "    resid_layer2_window_size,\n",
    "    num_layers=resid_layer2_num_layers,\n",
    "    regularizer=tf.keras.regularizers.l2(0.001)\n",
    ")\n",
    "\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    2,\n",
    "    1,\n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    ")(x)\n",
    "\n",
    "print(\"TYPPEPE\", type (x))\n",
    "x = tf.keras.layers.Lambda(\n",
    "    lambda x: drop_last_dim(x)\n",
    ")(x)\n",
    "\n",
    "print(\"TYPPEPE\", type (x))\n",
    "#     x = tf.keras.layers.Dropout(\n",
    "#         0.5,\n",
    "#         name=\"Drop-Out\"\n",
    "#     )(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    input_tensor,\n",
    "    x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (x.shape)#[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def drop_last_dim(x):\n",
    "    \"\"\"\n",
    "    Reshape a tensor of size (None, None, None, 1)\n",
    "    to (None, None, None)\n",
    "\n",
    "    :param x: input\n",
    "    :type  x: tensorflow tensor\n",
    "    :returns: reshaped input\n",
    "    :rtype:   tensorflow tensor\n",
    "    \"\"\"\n",
    "\n",
    "    x_shape = K.shape(x)\n",
    "    x_shape = x_shape[:-1]\n",
    "    return K.reshape(x, x_shape)\n",
    "\n",
    "#     K.concatenate([x_shape[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OuterProduct(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Given a layer of size (B, L, N), create \n",
    "    a layer of size (B, L, L, 3N).\n",
    "    If we have {v1, ..., vm},\n",
    "    for the i, j entry, we have (vi, v((i+j)/2), vj).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OuterProduct, self).__init__()\n",
    "\n",
    "    def call(self, incoming):\n",
    "        \"\"\"\n",
    "        Create the layer.\n",
    "\n",
    "        :param incoming: tensor of size (B, L, N)\n",
    "        :type  incoming: tensorflow.python.framework.ops.Tensor\n",
    "        :returns: tensor of size (B, L, L, 3N)\n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        L = tf.shape(incoming)[1]\n",
    "        # save the indexes of each position\n",
    "        v = tf.range(0, L, 1)\n",
    "\n",
    "        i, j = tf.meshgrid(v, v)\n",
    "\n",
    "        m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "        # switch batch dim with L dim to put L at first\n",
    "        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "        incoming2 = tf.Variable(incoming2)\n",
    "        # full matrix i with element in incomming2 indexed i[i][j]\n",
    "        out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "        out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "        out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "        # concatanate final feature dim together\n",
    "        out = tf.concat([out1, out2, out3], axis=3)\n",
    "        # return to original dims\n",
    "        output = tf.transpose(\n",
    "            out,\n",
    "            perm=[2, 0, 1, 3],\n",
    "            name=\"outer_product\"\n",
    "        )\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(np.linspace(6, 10, 5))\n",
    "y = tf.Variable(np.linspace(6, 10, 5))\n",
    "X,Y = tf.meshgrid(x, y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result1 = X.eval()\n",
    "    result2 = Y.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result1)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# (2, 100, 50)\n",
    "B = 1\n",
    "La = 5\n",
    "N = 1\n",
    "total_size = B * La * N\n",
    "incoming = tf.Variable(\n",
    "    np.linspace(1, total_size, total_size).reshape((B, La, N)),\n",
    "    dtype='int32'\n",
    ")\n",
    "L = tf.shape(incoming)[1]\n",
    "v = tf.range(0, L, 1)\n",
    "i, j = tf.meshgrid(v, v)\n",
    "m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "\n",
    "out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "\n",
    "out = tf.concat([out1, out2, out3], axis=3)\n",
    "# return to original dims\n",
    "output = tf.transpose(\n",
    "    out,\n",
    "    perm=[2, 0, 1, 3],\n",
    "    name=\"outer_product\"\n",
    ")\n",
    "\n",
    "\n",
    "out1a = tf.gather(incoming2, i)\n",
    "out2a = tf.gather(incoming2, j)\n",
    "out3a = tf.gather(incoming2, m)\n",
    "\n",
    "outa = tf.concat([out1a, out2a, out3a], axis=3)\n",
    "# return to original dims\n",
    "outputa = tf.transpose(\n",
    "    outa,\n",
    "    perm=[2, 0, 1, 3],\n",
    "    name=\"outer_product\"\n",
    ")\n",
    "\n",
    "# out1 = tf.dynamic_partition(incoming2, i, 4)[0]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "#     result1 = out1.eval()1\n",
    "#     result2 = out2.eval()\n",
    "#     result3 = out3.eval()\n",
    "    resulti = i.eval()\n",
    "    resultj = j.eval()\n",
    "    result_incoming2 = incoming2.eval()\n",
    "    resultout1 = out1.eval()\n",
    "    final_result = output.eval()\n",
    "    final_resulta = outputa.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_result == final_resulta\n",
    "# final_result\n",
    "resulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_incoming2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1 = tf.gather(incoming2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultout1#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1.shape\n",
    "# incoming.dtype\n",
    "result1.shape\n",
    "final_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input1 = tf.Variable(np.linspace(6,15, 10))\n",
    "indexes = tf.Variable(np.linspace(5,9,5), dtype='int32')\n",
    "gathered = tf.gather(input1, indexes)\n",
    "partitions = \n",
    "init2 = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init2.run()\n",
    "    result_gathered = gathered.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# (2, 100, 50)\n",
    "B = 2\n",
    "La = 50\n",
    "N = 6\n",
    "total_size = B * La * N\n",
    "incoming = tf.Variable(\n",
    "    np.linspace(1, total_size, total_size).reshape((B, La, N)),\n",
    "    dtype='int32'\n",
    ")\n",
    "L = tf.shape(incoming)[1]\n",
    "v = tf.range(0, L, 1)\n",
    "i, j = tf.meshgrid(v, v)\n",
    "m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result_incoming2 = incoming2.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_incoming2[:,1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.random.random((2, 5, 4),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array#[:, 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "second_index = randint (0, test_array.shape[1] - 1)\n",
    "third_index = randint (0, test_array.shape[2] - 1)\n",
    "test_arraya = test_array[:, second_index][:,third_index].reshape(-1, 1)\n",
    "test_arraya.shape\n",
    "final = np.tensordot (test_arraya, test_array.reshape((1,) + test_array.shape), axes=1)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalT = final.transpose([2, 0, 1, 3])\n",
    "finalT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "test_array =  tf.Variable (np.random.random((2, 5, 4),))\n",
    "# test_array = tf.placeholder(shape=(2, None, 4), dtype=\"int32\")\n",
    "L2 = tf.shape(test_array)[1]\n",
    "L3 = tf.shape(test_array)[2]\n",
    "rangeL2 = tf.range(0, L2, 1)\n",
    "rangeL3 = tf.range(0, L3, 1)\n",
    "\n",
    "index2 = tf.random.uniform ((1,), dtype=\"int32\", maxval= (L2) )[0]\n",
    "index3 = tf.random.uniform ((1,), dtype=\"int32\", maxval= (L3) )[0]\n",
    "\n",
    "\n",
    "# third_index = randint (0, test_array.shape[2] - 1)\n",
    "# print(test_array.get_shape)\n",
    "filler = test_array[:, index2][:,index3]\n",
    "# print(filler.dimension)\n",
    "# test_arraya = tf.reshape (filler, (-1, 1))#.reshape(-1, 1)\n",
    "test_arraya = tf.expand_dims (filler, 1)\n",
    "\n",
    "# test_array2 = tf.reshape(test_array, (1,) + test_array.shape )\n",
    "test_array2 = tf.expand_dims(test_array, 0)\n",
    "final = tf.tensordot (test_arraya, test_array2, axes=1)\n",
    "\n",
    "final2 = tf.transpose(\n",
    "    final,\n",
    "    perm=[2, 0, 1, 3],\n",
    "    name=\"outer_product\"\n",
    ")\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    random_num = index2.eval()\n",
    "    result_final = final2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_num[0]\n",
    "result_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrayb = test_array[1,:, 1].reshape(1, -1)\n",
    "test_arrayb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_arrayc = test_array.reshape(test_array.shape[:2] + (1,) + (test_array.shape[2], ))\n",
    "# test_arrayc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_array.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.tensordot(test_arrayc, test_arrayb, axes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OuterProduct2(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Given a layer of size (B, L, N), create \n",
    "    a layer of size (B, L, L, N).\n",
    "\n",
    "    This is done by:\n",
    "        switching B and L : (L, B, N)\n",
    "        extending the dimension: (1, L, B, N)\n",
    "        getting some random index: (L, 1)\n",
    "        computing tensorproduct: (L, 1) x (1, L, B, N)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OuterProduct2, self).__init__()\n",
    "\n",
    "    def call(self, incoming):\n",
    "        \"\"\"\n",
    "        Create the layer.\n",
    "\n",
    "        :param incoming: tensor of size (B, L, N)\n",
    "        :type  incoming: tensorflow.python.framework.ops.Tensor\n",
    "        :returns: tensor of size (B, L, L, N)\n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # switch batch dim with L dim to put L at first\n",
    "        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "        # get a random index value at tensor index 2 and 3\n",
    "        L2 = tf.shape(incoming2)[1]\n",
    "        L3 = tf.shape(incoming2)[2]\n",
    "        index2 = tf.random.uniform((1,), dtype=\"int32\", maxval=(L2))[0]\n",
    "        index3 = tf.random.uniform((1,), dtype=\"int32\", maxval=(L3))[0]\n",
    "\n",
    "        # compute the tensor product\n",
    "        inputa = tf.expand_dims(incoming2[:, index2][:, index3], 1)\n",
    "        incoming3 = tf.expand_dims(incoming2, 0)\n",
    "        tensorproduct = tf.tensordot(inputa, incoming3, axes=1)\n",
    "        tensorproduct = tf.transpose(\n",
    "            tensorproduct,\n",
    "            perm=[2, 0, 1, 3],\n",
    "            name=\"outer_product2\"\n",
    "        )\n",
    "\n",
    "        return tensorproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "incoming =  tf.Variable (np.random.random((2, 5, 4),))\n",
    "\n",
    "incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "L2 = tf.shape(incoming2)[1]\n",
    "L3 = tf.shape(incoming2)[2]\n",
    "index2 = tf.random.uniform((1,), dtype=\"int32\", maxval=(L2))[0]\n",
    "index3 = tf.random.uniform((1,), dtype=\"int32\", maxval=(L3))[0]\n",
    "\n",
    "\n",
    "# compute the tensor product\n",
    "inputa = tf.expand_dims(incoming2[:, index2][:, index3], 1)\n",
    "\n",
    "incoming3 = tf.expand_dims(incoming2, 0)\n",
    "\n",
    "tensorproduct = tf.tensordot(inputa, incoming3, axes=1)\n",
    "\n",
    "tensorproduct = tf.transpose(\n",
    "    tensorproduct,\n",
    "    perm=[2, 0, 1, 3],\n",
    "    name=\"outer_product2\"\n",
    ")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    random_num = index2.eval()\n",
    "    result_final = tensorproduct.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 5, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
