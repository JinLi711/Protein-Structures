{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the network used in:\n",
    "\n",
    "*Wang S, Sun S, Li Z, Zhang R, Xu J (2017) \n",
    "Accurate De Novo Prediction of Protein Contact Map by \n",
    "Ultra-Deep Learning Model. PLoS Comput Biol 13(1): e1005324. \n",
    "https://doi.org/10.1371/journal.pcbi.1005324*\n",
    "\n",
    "Here are some attributes they used in the paper:\n",
    "\n",
    "Inputs:\n",
    "    Along with 1 hot encoding of amino acids to a 20 dimension\n",
    "    1 hot encoding, they also included another 6 dimensions,\n",
    "    3-state secondary structure and 3-state solvent accessibility.\n",
    "    This was predicted using another neural network.\n",
    "    pairwise features: \n",
    "        mutual information, \n",
    "        the EC information calculated by CCMpred, \n",
    "        and pair- wise contact potential\n",
    "    were concatenated after the outer product layer\n",
    "\n",
    "Activation layer:\n",
    "    ReLU after every layer.\n",
    "    Batch normalization before activation layer.\n",
    "    (though did not say whether this was after \n",
    "    or before the convolution layer)\n",
    "\n",
    "Residual network:\n",
    "    the number of features of the next layer is greater or\n",
    "    equal to the one below it, so they had to pad\n",
    "    the previous layer with zeros to allow the skip adding.\n",
    "    For 1D residual network:\n",
    "        window size: 17 (fixed)\n",
    "        number of layers: 6 (fixed)\n",
    "    For 2D residual network:\n",
    "        window size: (3,3) or (5,5)\n",
    "        number of layers: ~60\n",
    "        number of hidden neurons per layer: ~60\n",
    "\n",
    "Loss function:\n",
    "    negative log-likelihood averaged over all \n",
    "    the residue pairs of the training proteins.\n",
    "    Since outputs were unbalanced, they assign a larger \n",
    "    weight to the residue pairs forming a contact.\n",
    "    The weight is assigned such that the total weight \n",
    "    assigned to contacts is approximately 1/8 of the number \n",
    "    of non-contacts in the training set.\n",
    "\n",
    "Mini-batches:\n",
    "    can have mini-batches, but they sorted the training set \n",
    "    and then grouped batches by related size. \n",
    "    Then they did some extra padding to make sure all proteins\n",
    "    in the batch had the same size.\n",
    "\n",
    "Others:\n",
    "    L2 normalization\n",
    "    stochastic gradient descent\n",
    "    drop out was never mentioned\n",
    "    20-30 epochs\n",
    "\n",
    "\n",
    "Things that I did not implement, even though I wish I did:\n",
    "    the extra six dimensions added to input\n",
    "    pairwise features\n",
    "    different layer sizes in residual network\n",
    "    used sparse categorical crossentropy instead of log\n",
    "    no weighing of outputs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Special Layers\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class OuterProduct(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Given a layer of size (B, L, N), create \n",
    "    a layer of size (B, L, L, 3N).\n",
    "    If we have {v1, ..., vm},\n",
    "    for the i, j entry, we have (vi, v((i+j)/2), vj).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterProduct, self).__init__()\n",
    "\n",
    "    def call(self, incoming):\n",
    "        \"\"\"\n",
    "        Create the layer.\n",
    "\n",
    "        :param incoming: tensor of size (B, L, N)\n",
    "        :type  incoming: tensorflow.python.framework.ops.Tensor\n",
    "        :returns: tensor of size (B, L, L, 3N)\n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        L = tf.shape(incoming)[1]\n",
    "        # save the indexes of each position\n",
    "        v = tf.range(0, L, 1)\n",
    "\n",
    "        i, j = tf.meshgrid(v, v)\n",
    "\n",
    "        m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "        # switch batch dim with L dim to put L at first\n",
    "        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "        # full matrix i with element in incomming2 indexed i[i][j]\n",
    "        out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "        out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "        out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "        # concatanate final feature dim together\n",
    "        out = tf.concat([out1, out2, out3], axis=3)\n",
    "        # return to original dims\n",
    "        output = tf.transpose(\n",
    "            out,\n",
    "            perm=[2, 0, 1, 3],\n",
    "            name=\"outer_product\"\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "def residual_conv_block(\n",
    "        x,\n",
    "        convnet,\n",
    "        stride,\n",
    "        num_layers,\n",
    "        regularizer=None,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"):\n",
    "    \"\"\"\n",
    "    Create a residual convolution block, either \n",
    "    in 1 or 2 dimensions.\n",
    "\n",
    "    :param x: \n",
    "    :type x:  \n",
    "    :param convnet: indicates which type of layer to use\n",
    "    :type  convnet: string\n",
    "    :param stride: stride\n",
    "    :type  stride: int\n",
    "    :param num_layers: number of layers for the entire residual network\n",
    "    :type  num_layers: int\n",
    "    :param regularizer: \n",
    "    :type  regularizer: \n",
    "    :param activation: \n",
    "    :type  activation: str\n",
    "    :param padding:\n",
    "    :type  padding: str\n",
    "    :returns: result of the residual network\n",
    "    :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    size = int(x.shape[-1])\n",
    "    y = x\n",
    "\n",
    "    if num_layers % 2 != 0:\n",
    "        raise ValueError(\"The number of layers must be even\")\n",
    "\n",
    "    def one_dim_block(x, i):\n",
    "        \"\"\"\n",
    "        Create the duo layer for conv1d.\n",
    "\n",
    "        :param x: input\n",
    "        :type  x: tensorflow.python.framework.ops.Tensor\n",
    "        :param i: position of that duo layer\n",
    "        :type  i: int\n",
    "        :returns: \n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        i += 1\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}a\".format(i),\n",
    "        )(x)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}a\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.Conv1D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}b\".format(i),\n",
    "        )(z)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}b\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.add(\n",
    "            [z, x],\n",
    "            name=convnet + \"_residual_block{}\".format(i)\n",
    "        )\n",
    "\n",
    "        return z\n",
    "\n",
    "    def two_dim_block(x, i):\n",
    "        \"\"\"\n",
    "        Create the duo layer for conv2d.\n",
    "\n",
    "        :param x: input\n",
    "        :type  x: tensorflow.python.framework.ops.Tensor\n",
    "        :param i: position of that duo layer\n",
    "        :type  i: int\n",
    "        :returns: \n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        i += 1\n",
    "        z = tf.keras.layers.Conv2D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}a\".format(i),\n",
    "        )(x)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}a\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.Conv2D(\n",
    "            size,\n",
    "            stride,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=regularizer,\n",
    "            name=convnet + \"_layer{}b\".format(i),\n",
    "        )(z)\n",
    "        z = tf.keras.layers.BatchNormalization(\n",
    "            name=convnet + \"_batch_norm{}b\".format(i),\n",
    "        )(z)\n",
    "\n",
    "        z = tf.keras.layers.add(\n",
    "            [z, x],\n",
    "            name=convnet + \"_residual_block{}\".format(i)\n",
    "        )\n",
    "\n",
    "        return z\n",
    "\n",
    "    if convnet == \"1d_convnet\":\n",
    "        for i in range(int(num_layers / 2)):\n",
    "            y = one_dim_block(y, i)\n",
    "\n",
    "    elif convnet == \"2d_convnet\":\n",
    "        for i in range(int(num_layers / 2)):\n",
    "            y = two_dim_block(y, i)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Not an available convnet dimension\")\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def inception_module (x):\n",
    "    \"\"\"\n",
    "    Create the inception V3 module\n",
    "    \n",
    "    :param x: Tensor input continuing from the chain\n",
    "    :type  x: tensorflow.python.framework.ops.Tensor\n",
    "    \n",
    "    :return: The concatenated output of the four branches\n",
    "    :rtype:  tensorflow.python.framework.ops.Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # from tensorflow.keras import layers\n",
    "\n",
    "    branch_a = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        1,\n",
    "        activation='relu',\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "\n",
    "    branch_b = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        1,\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    branch_b = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(branch_b)\n",
    "\n",
    "    branch_c = tf.keras.layers.AveragePooling2D(\n",
    "        3,\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    branch_c = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(branch_c)\n",
    "\n",
    "    branch_d = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        1,\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    branch_d = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        padding=\"same\"\n",
    "    )(branch_d)\n",
    "    branch_d = tf.keras.layers.Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        activation='relu',\n",
    "        strides=2,\n",
    "        padding=\"same\"\n",
    "    )(branch_d)\n",
    "\n",
    "    output = tf.keras.layers.concatenate(\n",
    "        [branch_a, branch_b, branch_c, branch_d],\n",
    "        name=\"Inception_V3\"\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Generator\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def aa_generator(x, y):\n",
    "    \"\"\"\n",
    "    Generator for feeding a single instance of an \n",
    "    input and an output.\n",
    "    The generator is reset when all elements are used.\n",
    "\n",
    "    :param: input\n",
    "    :type:  dict\n",
    "    :param: label\n",
    "    :type:  dict\n",
    "    :returns: a single instance of input and label\n",
    "    :rtype:   (numpy array, numpy array)\n",
    "    \"\"\"\n",
    "\n",
    "    keys = set(x.keys())\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            key = random.sample(keys, 1)[0]\n",
    "            keys.remove(key)\n",
    "\n",
    "            one_hot_aa = x[key]\n",
    "            one_hot_aa = np.reshape(\n",
    "                one_hot_aa, (1,) + one_hot_aa.shape\n",
    "            )\n",
    "            cmap = y[key]\n",
    "            cmap = np.reshape(cmap, (1,) + cmap.shape + (1,))\n",
    "            yield one_hot_aa, cmap\n",
    "\n",
    "        except ValueError:\n",
    "            # if out of keys, reinsert back the keys\n",
    "            keys = set(x.keys())\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Create the model\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_architecture(\n",
    "    resid_layer2_window_size, \n",
    "    resid_layer2_num_layers):\n",
    "    \"\"\"\n",
    "    Create the basic architecture. \n",
    "    1d residual network followed by 2d residual network.\n",
    "\n",
    "    :param resid_layer2_window_size: window size\n",
    "    :type  resid_layer2_window_size: int\n",
    "    :param resid_layer2_num_layers: number of layers\n",
    "    :type  resid_layer2_num_layers: int\n",
    "    :returns: training model\n",
    "    :rtype:   tensorflow.python.keras.engine.training.Model\n",
    "    \"\"\"\n",
    "\n",
    "    input_tensor = tf.keras.Input(\n",
    "        shape=(None, 20),\n",
    "        name=\"input_layer\"\n",
    "    )\n",
    "\n",
    "    x = residual_conv_block(\n",
    "        input_tensor,\n",
    "        \"1d_convnet\",\n",
    "        17,\n",
    "        num_layers=6,\n",
    "        regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )\n",
    "\n",
    "    x = OuterProduct(\n",
    "    )(x)\n",
    "\n",
    "    x = residual_conv_block(\n",
    "        x,\n",
    "        \"2d_convnet\",\n",
    "        resid_layer2_window_size,\n",
    "        num_layers=resid_layer2_num_layers,\n",
    "        regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        2,\n",
    "        1,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(x)\n",
    "\n",
    "#     x = tf.keras.layers.Dropout(\n",
    "#         0.5,\n",
    "#         name=\"Drop-Out\"\n",
    "#     )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        input_tensor,\n",
    "        x\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Callbacks\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "weight_path = \"best_weight/{}_weights.best.hdf5\".format(\n",
    "    'Double_Resid_Network'\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    weight_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# tensorboard = tf.keras.callback.TensorBoard(\n",
    "#     log_dir='Logs',\n",
    "# )\n",
    "\n",
    "reduceLROnPlat = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    \"\"\"\n",
    "    Reduce learning rate after epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    import math \n",
    "\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(\n",
    "        drop, math.floor((1+epoch)/epochs_drop)\n",
    "    )\n",
    "    return lrate\n",
    "\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    # monitor='acc'\n",
    "    mode=\"min\",\n",
    "    verbose=2,\n",
    "    # training is interrupted when the monitor argument \n",
    "    # stops improving after n steps\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     import sys\n",
    "\n",
    "#     data_path = \"../../data/cull%i/model_data/\" % int (sys.argv[1])\n",
    "\n",
    "#     train_aa_dict = np.load(data_path + 'train_aa_dict.npy')[()]\n",
    "#     train_cmap_dict = np.load(data_path + 'train_cmap_dict.npy')[()]\n",
    "#     valid_aa_dict = np.load(data_path + 'valid_aa_dict.npy')[()]\n",
    "#     valid_cmap_dict = np.load(data_path + 'valid_cmap_dict.npy')[()]\n",
    "#     devtest_aa_dict = np.load(data_path + 'devtest_aa_dict.npy')[()]\n",
    "#     devtest_cmap_dict = np.load(data_path + 'devtest_cmap_dict.npy')[()]\n",
    "\n",
    "\n",
    "#     model = create_architecture(3, 60)\n",
    "#     print (model.summary())\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=\"adam\",\n",
    "#         loss=\"sparse_categorical_crossentropy\",\n",
    "#         sample_weight_mode=\"temporal\",\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "\n",
    "#     history = model.fit_generator(\n",
    "#         aa_generator(train_aa_dict, train_cmap_dict),\n",
    "#         validation_data=aa_generator(valid_aa_dict, valid_cmap_dict),\n",
    "#         steps_per_epoch=len(train_aa_dict), \n",
    "#         epochs=20,\n",
    "#         validation_steps=10,\n",
    "#         callbacks=callbacks_list\n",
    "#     )\n",
    "\n",
    "#     model.save('my_model.h5') \n",
    "\n",
    "#     # to load back the model:\n",
    "#     # model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OuterProduct(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Given a layer of size (B, L, N), create \n",
    "    a layer of size (B, L, L, 3N).\n",
    "    If we have {v1, ..., vm},\n",
    "    for the i, j entry, we have (vi, v((i+j)/2), vj).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterProduct, self).__init__()\n",
    "\n",
    "    def call(self, incoming):\n",
    "        \"\"\"\n",
    "        Create the layer.\n",
    "\n",
    "        :param incoming: tensor of size (B, L, N)\n",
    "        :type  incoming: tensorflow.python.framework.ops.Tensor\n",
    "        :returns: tensor of size (B, L, L, 3N)\n",
    "        :rtype:   tensorflow.python.framework.ops.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        L = tf.shape(incoming)[1]\n",
    "        # save the indexes of each position\n",
    "        v = tf.range(0, L, 1)\n",
    "\n",
    "        i, j = tf.meshgrid(v, v)\n",
    "        print(i.shape)\n",
    "\n",
    "        m = tf.cast((i+j)/2, tf.int32)\n",
    "\n",
    "        # switch batch dim with L dim to put L at first\n",
    "        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n",
    "        \n",
    "#         incoming2 = tf.sparse.transpose(incoming, perm=[1, 0, 2])\n",
    "\n",
    "        # full matrix i with element in incomming2 indexed i[i][j]\n",
    "#         print(incoming2.op)\n",
    "        out1 = tf.nn.embedding_lookup(incoming2, i)\n",
    "        out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "        out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "#         out1 = tf.gather(incoming2, i)\n",
    "#         out1 = tf.dynamic_partition(incoming2, i, 4)[0]\n",
    "#         out2 = tf.dynamic_partition(incoming2, j, 4)[0]\n",
    "#         out3 = tf.dynamic_partition(incoming2, m, 4)[0]\n",
    "#         out2 = tf.nn.embedding_lookup(incoming2, j)\n",
    "#         out3 = tf.nn.embedding_lookup(incoming2, m)\n",
    "\n",
    "        # concatanate final feature dim together\n",
    "        out = tf.concat([out1, out2, out3], axis=3)\n",
    "        # return to original dims\n",
    "        output = tf.transpose(\n",
    "            out,\n",
    "            perm=[2, 0, 1, 3],\n",
    "            name=\"outer_product\"\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"input_layer_22\"\n",
      "op: \"Placeholder\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 20\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INCOMING:  name: \"1d_convnet_residual_block3_22/add\"\n",
      "op: \"Add\"\n",
      "input: \"1d_convnet_batch_norm3b_22/batchnorm/add_1\"\n",
      "input: \"1d_convnet_residual_block2_22/add\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n",
      "(?, ?)\n"
     ]
    }
   ],
   "source": [
    "resid_layer2_window_size = 3\n",
    "resid_layer2_num_layers = 2\n",
    "\n",
    "input_tensor = tf.keras.Input(\n",
    "    shape=(None, 20),\n",
    "    name=\"input_layer\"\n",
    ")\n",
    "\n",
    "print(input_tensor.op)\n",
    "x = residual_conv_block(\n",
    "    input_tensor,\n",
    "    \"1d_convnet\",\n",
    "    17,\n",
    "    num_layers=6,\n",
    "    regularizer=tf.keras.regularizers.l2(0.001)\n",
    ")\n",
    "\n",
    "print(\"INCOMING: \", x.op )\n",
    "x = OuterProduct(\n",
    ")(x)\n",
    "\n",
    "x = residual_conv_block(\n",
    "    x,\n",
    "    \"2d_convnet\",\n",
    "    resid_layer2_window_size,\n",
    "    num_layers=resid_layer2_num_layers,\n",
    "    regularizer=tf.keras.regularizers.l2(0.001)\n",
    ")\n",
    "\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    2,\n",
    "    1,\n",
    "    activation='relu',\n",
    "    padding='same',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    ")(x)\n",
    "\n",
    "#     x = tf.keras.layers.Dropout(\n",
    "#         0.5,\n",
    "#         name=\"Drop-Out\"\n",
    "#     )(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    input_tensor,\n",
    "    x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
