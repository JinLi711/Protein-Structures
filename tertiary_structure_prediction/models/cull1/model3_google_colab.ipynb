{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"1Eh46aYjYMc7","colab_type":"code","outputId":"8c93fce0-c915-48fa-ebd4-4a6f90217067","executionInfo":{"status":"ok","timestamp":1553904002352,"user_tz":240,"elapsed":34316,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!pwd"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content\n"],"name":"stdout"}]},{"metadata":{"id":"VtQ_-fyXY5KH","colab_type":"code","outputId":"6959611b-c394-4a6e-80aa-7b90d644ec91","executionInfo":{"status":"ok","timestamp":1553904003667,"user_tz":240,"elapsed":34406,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["init_path = \"/content/gdrive/My\\ Drive/Colab\\ Notebooks/tertiary_structure_prediction/models/\"\n","\n","%cd /content\n","%cd gdrive/My\\ Drive/Colab\\ Notebooks/Protein_Structures/tertiary_structure_prediction/models/cull1/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","/content/gdrive/My Drive/Colab Notebooks/Protein_Structures/tertiary_structure_prediction/models/cull1\n"],"name":"stdout"}]},{"metadata":{"id":"YlVQzC2mXndL","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","import numpy as np\n","# sys.path.insert(0, '../model_functions')\n","# sys.path.insert(0, init_path + \"model_functions\")\n","# %cd gdrive/My\\ Drive/Colab\\ Notebooks/Protein_Structures/tertiary_structure_prediction/models/model_functions/\n","# import primary_model as pm\n","# import importlib\n","# importlib.reload(pm)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gEhdZJ06eSVR","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","This module contains the network used in:\n","*Wang S, Sun S, Li Z, Zhang R, Xu J (2017) \n","Accurate De Novo Prediction of Protein Contact Map by \n","Ultra-Deep Learning Model. PLoS Comput Biol 13(1): e1005324. \n","https://doi.org/10.1371/journal.pcbi.1005324*\n","Here are some attributes they used in the paper:\n","Inputs:\n","    Along with 1 hot encoding of amino acids to a 20 dimension\n","    1 hot encoding, they also included another 6 dimensions,\n","    3-state secondary structure and 3-state solvent accessibility.\n","    This was predicted using another neural network.\n","    pairwise features: \n","        mutual information, \n","        the EC information calculated by CCMpred, \n","        and pair- wise contact potential\n","    were concatenated after the outer product layer\n","Activation layer:\n","    ReLU after every layer.\n","    Batch normalization before activation layer.\n","    (though did not say whether this was after \n","    or before the convolution layer)\n","Residual network:\n","    the number of features of the next layer is greater or\n","    equal to the one below it, so they had to pad\n","    the previous layer with zeros to allow the skip adding.\n","    For 1D residual network:\n","        window size: 17 (fixed)\n","        number of layers: 6 (fixed)\n","    For 2D residual network:\n","        window size: (3,3) or (5,5)\n","        number of layers: ~60\n","        number of hidden neurons per layer: ~60\n","Loss function:\n","    negative log-likelihood averaged over all \n","    the residue pairs of the training proteins.\n","    Since outputs were unbalanced, they assign a larger \n","    weight to the residue pairs forming a contact.\n","    The weight is assigned such that the total weight \n","    assigned to contacts is approximately 1/8 of the number \n","    of non-contacts in the training set.\n","Mini-batches:\n","    can have mini-batches, but they sorted the training set \n","    and then grouped batches by related size. \n","    Then they did some extra padding to make sure all proteins\n","    in the batch had the same size.\n","Others:\n","    L2 normalization\n","    stochastic gradient descent\n","    drop out was never mentioned\n","    20-30 epochs\n","Things that I did not implement, even though I wish I did:\n","    the extra six dimensions added to input\n","    pairwise features\n","    different layer sizes in residual network\n","    used sparse categorical crossentropy instead of log\n","    no weighing of outputs\n","    did not have 60 layers for second residual network. \n","    (Too many parameters, memory exploded, not sure \n","    how to deal with this yet).\n","    Created my own Outer Product layer\n","\"\"\"\n","\n","\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import random\n","import tensorflow as tf\n","\n","\n","#----------------------------------------------------------------------\n","# Special Layers\n","#----------------------------------------------------------------------\n","\n","\n","class OuterProduct(tf.keras.layers.Layer):\n","    \"\"\"\n","    Given a layer of size (B, L, N), create \n","    a layer of size (B, L, L, 3N).\n","    If we have {v1, ..., vm},\n","    for the i, j entry, we have (vi, v((i+j)/2), vj).\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(OuterProduct, self).__init__()\n","\n","    def call(self, incoming):\n","        \"\"\"\n","        Create the layer.\n","        :param incoming: tensor of size (B, L, N)\n","        :type  incoming: tensorflow.python.framework.ops.Tensor\n","        :returns: tensor of size (B, L, L, 3N)\n","        :rtype:   tensorflow.python.framework.ops.Tensor\n","        \"\"\"\n","\n","        L = tf.shape(incoming)[1]\n","        # save the indexes of each position\n","        v = tf.range(0, L, 1)\n","\n","        i, j = tf.meshgrid(v, v)\n","\n","        m = tf.cast((i+j)/2, tf.int32)\n","\n","        # switch batch dim with L dim to put L at first\n","        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n","\n","        # full matrix i with element in incomming2 indexed i[i][j]\n","        out1 = tf.nn.embedding_lookup(incoming2, i)\n","        out2 = tf.nn.embedding_lookup(incoming2, j)\n","        out3 = tf.nn.embedding_lookup(incoming2, m)\n","\n","        # concatanate final feature dim together\n","        out = tf.concat([out1, out2, out3], axis=3)\n","        # return to original dims\n","        output = tf.transpose(\n","            out,\n","            perm=[2, 0, 1, 3],\n","            name=\"outer_product\"\n","        )\n","        return output\n","\n","\n","class OuterProduct2(tf.keras.layers.Layer):\n","    \"\"\"\n","    Given a layer of size (B, L, N), create \n","    a layer of size (B, L, L, N).\n","    This is done by:\n","        switching B and L : (L, B, N)\n","        extending the dimension: (1, L, B, N)\n","        getting some random index: (L, 1)\n","        computing tensorproduct: (L, 1) x (1, L, B, N)\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(OuterProduct2, self).__init__()\n","\n","    def call(self, incoming):\n","        \"\"\"\n","        Create the layer.\n","        :param incoming: tensor of size (B, L, N)\n","        :type  incoming: tensorflow.python.framework.ops.Tensor\n","        :returns: tensor of size (B, L, L, N)\n","        :rtype:   tensorflow.python.framework.ops.Tensor\n","        \"\"\"\n","\n","        # switch batch dim with L dim to put L at first\n","        incoming2 = tf.transpose(incoming, perm=[1, 0, 2])\n","\n","        # get a random index value at tensor index 2 and 3\n","        L2 = tf.shape(incoming2)[1]\n","        L3 = tf.shape(incoming2)[2]\n","        index2 = tf.random.uniform((1,), dtype=\"int32\", maxval=(L2))[0]\n","        index3 = tf.random.uniform((1,), dtype=\"int32\", maxval=(L3))[0]\n","\n","        # compute the tensor product\n","        inputa = tf.expand_dims(incoming2[:, index2][:, index3], 1)\n","        incoming3 = tf.expand_dims(incoming2, 0)\n","        tensorproduct = tf.tensordot(inputa, incoming3, axes=1)\n","        tensorproduct = tf.transpose(\n","            tensorproduct,\n","            perm=[2, 0, 1, 3],\n","            name=\"outer_product2\"\n","        )\n","\n","        return tensorproduct\n","\n","\n","def residual_conv_block(\n","        x,\n","        convnet,\n","        stride,\n","        num_layers,\n","        regularizer=None,\n","        activation=\"relu\",\n","        padding=\"same\"):\n","    \"\"\"\n","    Create a residual convolution block, either \n","    in 1 or 2 dimensions.\n","    :param x: \n","    :type x:  \n","    :param convnet: indicates which type of layer to use\n","    :type  convnet: string\n","    :param stride: stride\n","    :type  stride: int\n","    :param num_layers: number of layers for the entire residual network\n","    :type  num_layers: int\n","    :param regularizer: \n","    :type  regularizer: \n","    :param activation: \n","    :type  activation: str\n","    :param padding:\n","    :type  padding: str\n","    :returns: result of the residual network\n","    :rtype:   tensorflow.python.framework.ops.Tensor\n","    \"\"\"\n","\n","    size = int(x.shape[-1])\n","    y = x\n","\n","    if num_layers % 2 != 0:\n","        raise ValueError(\"The number of layers must be even\")\n","\n","    def one_dim_block(x, i):\n","        \"\"\"\n","        Create the duo layer for conv1d.\n","        :param x: input\n","        :type  x: tensorflow.python.framework.ops.Tensor\n","        :param i: position of that duo layer\n","        :type  i: int\n","        :returns: \n","        :rtype:   tensorflow.python.framework.ops.Tensor\n","        \"\"\"\n","\n","        i += 1\n","        z = tf.keras.layers.Conv1D(\n","            size,\n","            stride,\n","            activation=activation,\n","            padding=padding,\n","            kernel_regularizer=regularizer,\n","            name=convnet + \"_layer{}a\".format(i),\n","        )(x)\n","        z = tf.keras.layers.BatchNormalization(\n","            name=convnet + \"_batch_norm{}a\".format(i),\n","        )(z)\n","\n","        z = tf.keras.layers.Conv1D(\n","            size,\n","            stride,\n","            activation=activation,\n","            padding=padding,\n","            kernel_regularizer=regularizer,\n","            name=convnet + \"_layer{}b\".format(i),\n","        )(z)\n","        z = tf.keras.layers.BatchNormalization(\n","            name=convnet + \"_batch_norm{}b\".format(i),\n","        )(z)\n","\n","        z = tf.keras.layers.add(\n","            [z, x],\n","            name=convnet + \"_residual_block{}\".format(i)\n","        )\n","\n","        return z\n","\n","    def two_dim_block(x, i):\n","        \"\"\"\n","        Create the duo layer for conv2d.\n","        :param x: input\n","        :type  x: tensorflow.python.framework.ops.Tensor\n","        :param i: position of that duo layer\n","        :type  i: int\n","        :returns: \n","        :rtype:   tensorflow.python.framework.ops.Tensor\n","        \"\"\"\n","\n","        i += 1\n","        z = tf.keras.layers.Conv2D(\n","            size,\n","            stride,\n","            activation=activation,\n","            padding=padding,\n","            kernel_regularizer=regularizer,\n","            name=convnet + \"_layer{}a\".format(i),\n","        )(x)\n","        z = tf.keras.layers.BatchNormalization(\n","            name=convnet + \"_batch_norm{}a\".format(i),\n","        )(z)\n","\n","        z = tf.keras.layers.Conv2D(\n","            size,\n","            stride,\n","            activation=activation,\n","            padding=padding,\n","            kernel_regularizer=regularizer,\n","            name=convnet + \"_layer{}b\".format(i),\n","        )(z)\n","        z = tf.keras.layers.BatchNormalization(\n","            name=convnet + \"_batch_norm{}b\".format(i),\n","        )(z)\n","\n","        z = tf.keras.layers.add(\n","            [z, x],\n","            name=convnet + \"_residual_block{}\".format(i)\n","        )\n","\n","        return z\n","\n","    if convnet == \"1d_convnet\":\n","        for i in range(int(num_layers / 2)):\n","            y = one_dim_block(y, i)\n","\n","    elif convnet == \"2d_convnet\":\n","        for i in range(int(num_layers / 2)):\n","            y = two_dim_block(y, i)\n","\n","    else:\n","        raise ValueError(\"Not an available convnet dimension\")\n","\n","    return y\n","\n","\n","def inception_module (x):\n","    \"\"\"\n","    Create the inception V3 module\n","    \n","    :param x: Tensor input continuing from the chain\n","    :type  x: tensorflow.python.framework.ops.Tensor\n","    \n","    :return: The concatenated output of the four branches\n","    :rtype:  tensorflow.python.framework.ops.Tensor\n","    \"\"\"\n","\n","    # from tensorflow.keras import layers\n","\n","    branch_a = tf.keras.layers.Conv2D(\n","        128,\n","        1,\n","        activation='relu',\n","        strides=2,\n","        padding=\"same\"\n","    )(x)\n","\n","    branch_b = tf.keras.layers.Conv2D(\n","        128,\n","        1,\n","        activation='relu'\n","    )(x)\n","    branch_b = tf.keras.layers.Conv2D(\n","        128,\n","        3,\n","        activation='relu',\n","        strides=2,\n","        padding=\"same\"\n","    )(branch_b)\n","\n","    branch_c = tf.keras.layers.AveragePooling2D(\n","        3,\n","        strides=2,\n","        padding=\"same\"\n","    )(x)\n","    branch_c = tf.keras.layers.Conv2D(\n","        128,\n","        3,\n","        activation='relu',\n","        padding=\"same\"\n","    )(branch_c)\n","\n","    branch_d = tf.keras.layers.Conv2D(\n","        128,\n","        1,\n","        activation='relu'\n","    )(x)\n","    branch_d = tf.keras.layers.Conv2D(\n","        128,\n","        3,\n","        activation='relu',\n","        padding=\"same\"\n","    )(branch_d)\n","    branch_d = tf.keras.layers.Conv2D(\n","        128,\n","        3,\n","        activation='relu',\n","        strides=2,\n","        padding=\"same\"\n","    )(branch_d)\n","\n","    output = tf.keras.layers.concatenate(\n","        [branch_a, branch_b, branch_c, branch_d],\n","        name=\"Inception_V3\"\n","    )\n","    return output\n","\n","\n","#----------------------------------------------------------------------\n","# Functions Wrapped in Lambda Layers\n","#----------------------------------------------------------------------\n","\n","\n","def drop_last_dim(x):\n","    \"\"\"\n","    Reshape a tensor of size (None, None, None, 1)\n","    to (None, None, None)\n","    :param x: input\n","    :type  x: tensorflow tensor\n","    :returns: reshaped input\n","    :rtype:   tensorflow tensor\n","    \"\"\"\n","\n","    x_shape = tf.keras.backend.shape(x)\n","    x_shape = x_shape[:-1]\n","    return tf.keras.backend.reshape(x, x_shape)\n","\n","\n","#----------------------------------------------------------------------\n","# Generators\n","#----------------------------------------------------------------------\n","\n","\n","def aa_generator_batch(x, y, batch_size=1):\n","    \"\"\"\n","    A generator for batches of sorted size.\n","    :param x: dictionary mapping keys to aa 1 hot\n","    :type  x: dict\n","    :param y: dictionary mapping keys to cmaps\n","    :type  y: dict\n","    :param batch_size: size of batch\n","    :type  batch_size: int\n","    :returns: aa batch, cmap batch\n","    :rtype:   (np.array, np.array)\n","    \"\"\"\n","\n","    aa = [(pdb_id, array) for (pdb_id, array) in x.items()]\n","    aa.sort(\n","        key=lambda x: x[1].shape[0],\n","        reverse=True\n","    )\n","    cmaps = [(pdb_id, array) for (pdb_id, array) in y.items()]\n","    cmaps.sort(\n","        key=lambda x: x[1].shape[0],\n","        reverse=True\n","    )\n","\n","    def check_if_aligned(x, y):\n","        \"\"\"\n","        Check if the sorting is aligned.\n","        :param x: list of tuples (a, c)\n","        :type  x: list\n","        :param y: list of tuples (a, b)\n","        :type  y: list\n","        \"\"\"\n","\n","        if len(x) != len(y):\n","            raise ValueError(\"Lengths do not match.\")\n","\n","        for i in range(len(x)):\n","            if (x[i][0] != y[i][0]):\n","                raise ValueError(\"Not sorted correctly\")\n","\n","    check_if_aligned(aa, cmaps)\n","\n","    def create_batches(aa, cmaps, batch_size):\n","        \"\"\"\n","        Create the batches.\n","        :param aa: list of tuple (pdb_id, aa 1 hot)\n","        :type  aa: list\n","        :param cmaps: list of tuple (pdb_id, cmap)\n","        :type  cmaps: list\n","        :param batch_size: size of batch\n","        :type  batch_size: int\n","        :returns: dictionary mapping a key to a tuple\n","                  (aa batch, cmap batch)\n","        :rtype:   dict\n","        \"\"\"\n","\n","        all_batches = {}\n","        for i in range(0, len(aa), batch_size):\n","            aa_batch = aa[i:i+batch_size]\n","            cmap_batch = cmaps[i:i+batch_size]\n","            max_length = aa[i][1].shape[0]\n","\n","            aa_batch = [\n","                np.pad(\n","                    array,\n","                    ((0, max_length - array.shape[0]),\n","                     (0, 0)),\n","                    'constant')\n","                for (pdb_id, array) in aa_batch\n","            ]\n","\n","            cmap_batch = [\n","                np.pad(\n","                    array,\n","                    ((0, max_length - array.shape[0]),\n","                     (0, max_length - array.shape[1])),\n","                    'constant')\n","                for (pdb_id, array) in cmap_batch\n","            ]\n","\n","            # example:\n","            # shape of (4,5,6) becomes (4,30)\n","            # equivalent to tf.flatten\n","            cmap_batch = [\n","                batch.flatten()\n","                for batch in cmap_batch\n","            ]  \n","\n","            # shape of (4,30) becomes (4,30,1)\n","            cmap_batch = [\n","                np.expand_dims(batch, axis=2)\n","                for batch in cmap_batch\n","            ]\n","            \n","                \n","            stacked_aa_batch = np.stack(aa_batch, axis=0)\n","            stacked_cmap_batch = np.stack(cmap_batch, axis=0)\n","\n","            all_batches[\"batch\" +\n","                        str(i)] = (stacked_aa_batch, stacked_cmap_batch)\n","\n","        return all_batches\n","\n","    all_batches = create_batches(aa, cmaps, batch_size)\n","    keys = set(all_batches.keys())\n","    \n","\n","    while True:\n","        try:\n","            key = random.sample(keys, 1)[0]\n","            keys.remove(key)\n","            aa_batch, cmap_batch = all_batches[key]\n","\n","\n","            class_weight = np.zeros(\n","                cmap_batch.shape\n","            )\n","\n","            class_weight += cmap_batch * 8\n","            class_weight[class_weight == 0] = 1\n","            class_weight = class_weight.reshape(\n","                (class_weight.shape[:2])\n","            )\n","\n","            yield aa_batch, cmap_batch, class_weight\n","\n","        except ValueError:\n","            # if out of keys, reinsert back the keys\n","            keys = set(all_batches.keys())\n","\n","\n","#----------------------------------------------------------------------\n","# Create the model\n","#----------------------------------------------------------------------\n","\n","\n","def create_architecture(\n","    resid_layer2_window_size, \n","    resid_layer2_num_layers):\n","    \"\"\"\n","    Create the basic architecture. \n","    1d residual network followed by 2d residual network.\n","    :param resid_layer2_window_size: window size\n","    :type  resid_layer2_window_size: int\n","    :param resid_layer2_num_layers: number of layers\n","    :type  resid_layer2_num_layers: int\n","    :returns: training model\n","    :rtype:   tensorflow.python.keras.engine.training.Model\n","    \"\"\"\n","\n","    input_tensor = tf.keras.Input(\n","        shape=(None, 20),\n","        name=\"input_layer\"\n","    )\n","\n","    x = residual_conv_block(\n","        input_tensor,\n","        \"1d_convnet\",\n","        17,\n","        num_layers=6,\n","        regularizer=tf.keras.regularizers.l2(0.001)\n","    )\n","\n","    x = OuterProduct(\n","    )(x)\n","\n","    # x = tf.keras.layers.Conv2D(\n","    #     60,\n","    #     1,\n","    #     activation='relu',\n","    #     padding='same',\n","    #     kernel_regularizer=tf.keras.regularizers.l2(0.001)\n","    # )(x)\n","\n","    x = residual_conv_block(\n","        x,\n","        \"2d_convnet\",\n","        resid_layer2_window_size,\n","        num_layers=resid_layer2_num_layers,\n","        regularizer=tf.keras.regularizers.l2(0.001)\n","    )\n","\n","    x = tf.keras.layers.Conv2D(\n","        1,\n","        1,\n","        activation='relu',\n","        padding='same',\n","        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n","    )(x)\n","\n","    x = tf.keras.layers.BatchNormalization(\n","    )(x)\n","\n","    x = tf.keras.layers.Flatten(\n","    )(x)\n","\n","    # x = tf.expand_dims(\n","    #     x,\n","    #     axis=2\n","    # )\n","\n","    x = tf.keras.layers.Lambda(\n","        lambda x: tf.expand_dims(x, axis=2)\n","    )(x)\n","\n","    x = tf.keras.layers.Activation(\n","        activation=\"sigmoid\"\n","    )(x)\n","\n","    # x = tf.keras.layers.Lambda(\n","    #     lambda x: drop_last_dim(x)\n","    # )(x)\n","\n","    # x = tf.keras.layers.Dropout(\n","    #     0.5,\n","    #     name=\"Drop-Out\"\n","    # )(x)\n","\n","    model = tf.keras.models.Model(\n","        input_tensor,\n","        x\n","    )\n","\n","    return model\n","\n","\n","#----------------------------------------------------------------------\n","# Callbacks\n","#----------------------------------------------------------------------\n","\n","\n","weight_path = \"best_weight/{}_weights.best.hdf5\".format(\n","    'Double_Resid_Network'\n",")\n","\n","\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    weight_path,\n","    monitor='val_loss',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='min',\n","    save_weights_only=True\n",")\n","\n","tensorboard = tf.keras.callbacks.TensorBoard(\n","    log_dir='Logs',\n",")\n","\n","reduceLROnPlat = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,\n","    patience=2,\n","    verbose=1,\n","    mode='min',\n","    min_delta=0.0001,\n","    cooldown=2,\n","    min_lr=1e-7\n",")\n","\n","\n","def step_decay(epoch):\n","    \"\"\"\n","    Reduce learning rate after epochs.\n","    \"\"\"\n","\n","    import math \n","\n","    initial_lrate = 0.0001\n","    drop = 0.5\n","    epochs_drop = 10.0\n","    lrate = initial_lrate * math.pow(\n","        drop, math.floor((1+epoch)/epochs_drop)\n","    )\n","    return lrate\n","\n","lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n","\n","early = tf.keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\",\n","    # monitor='acc'\n","    mode=\"min\",\n","    verbose=2,\n","    # training is interrupted when the monitor argument \n","    # stops improving after n steps\n","    patience=15\n",")\n","\n","callbacks_list = [\n","    checkpoint, \n","    early, \n","    reduceLROnPlat,\n","    tensorboard\n","]\n","\n","\n","\n","# if __name__ == \"__main__\":\n","\n","#     import sys\n","\n","#     data_path = \"../../data/cull%i/model_data/\" % int (sys.argv[1])\n","\n","#     train_aa_dict = np.load(data_path + 'train_aa_dict.npy')[()]\n","#     train_cmap_dict = np.load(data_path + 'train_cmap_dict.npy')[()]\n","#     valid_aa_dict = np.load(data_path + 'valid_aa_dict.npy')[()]\n","#     valid_cmap_dict = np.load(data_path + 'valid_cmap_dict.npy')[()]\n","#     devtest_aa_dict = np.load(data_path + 'devtest_aa_dict.npy')[()]\n","#     devtest_cmap_dict = np.load(data_path + 'devtest_cmap_dict.npy')[()]\n","\n","\n","#     model = create_architecture(3, 60)\n","#     print (model.summary())\n","\n","#     batch_size = 1\n","#     epochs=20\n","#     train_steps = round(len(train_aa_dict) / batch_size)\n","#     valid_steps = round(len(valid_aa_dict) / batch_size)\n","\n","#     model.compile(\n","#         optimizer=\"adam\",\n","#         loss=\"binary_crossentropy\",\n","#         sample_weight_mode=\"temporal\",\n","#         metrics=['accuracy']\n","#     )\n","\n","#     history = model.fit_generator(\n","#         aa_generator_batch(\n","#             train_aa_dict, \n","#             train_cmap_dict, \n","#             batch_size),\n","#         validation_data=aa_generator_batch(\n","#             valid_aa_dict, \n","#             valid_cmap_dict, \n","#             batch_size),\n","#         steps_per_epoch=train_steps,\n","#         epochs=epochs,\n","#         validation_steps=valid_steps,\n","#         callbacks=callbacks_list,\n","#     )\n","\n","#     model.save('my_model.h5') \n","\n","#     # to load back the model:\n","#     # model = tf.keras.models.load_model('my_model.h5')\n","\n","#     # note: make sure the versions are correct\n","#     # if the model is saved from tensorflow 1.12.0,\n","#     # it should not be loaded into a python program\n","#     # using tensorflow 1.13.0\n","\n","#     # need to also call the custom layers when loading"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4iC3GIKsXndT","colab_type":"code","colab":{}},"cell_type":"code","source":["data_path = \"../../data/cull%i/model_data/\" % 2\n","\n","devtest_aa_dict = np.load(data_path + 'devtest_aa_dict.npy')[()]\n","devtest_cmap_dict = np.load(data_path + 'devtest_cmap_dict.npy')[()]\n","train_aa_dict = np.load(data_path + 'train_aa_dict.npy')[()]\n","train_cmap_dict = np.load(data_path + 'train_cmap_dict.npy')[()]\n","valid_aa_dict = np.load(data_path + 'valid_aa_dict.npy')[()]\n","valid_cmap_dict = np.load(data_path + 'valid_cmap_dict.npy')[()]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pGmh3zViXndW","colab_type":"code","colab":{}},"cell_type":"code","source":["model = create_architecture(3, 14)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q3l4SBQmXndZ","colab_type":"code","outputId":"9fa7a946-20c8-4e24-fde5-9da18a27f6a1","executionInfo":{"status":"ok","timestamp":1553906207881,"user_tz":240,"elapsed":295,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}},"colab":{"base_uri":"https://localhost:8080/","height":2218}},"cell_type":"code","source":["model.summary()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_layer (InputLayer)        (None, None, 20)     0                                            \n","__________________________________________________________________________________________________\n","1d_convnet_layer1a (Conv1D)     (None, None, 20)     6820        input_layer[0][0]                \n","__________________________________________________________________________________________________\n","1d_convnet_batch_norm1a (BatchN (None, None, 20)     80          1d_convnet_layer1a[0][0]         \n","__________________________________________________________________________________________________\n","1d_convnet_layer1b (Conv1D)     (None, None, 20)     6820        1d_convnet_batch_norm1a[0][0]    \n","__________________________________________________________________________________________________\n","1d_convnet_batch_norm1b (BatchN (None, None, 20)     80          1d_convnet_layer1b[0][0]         \n","__________________________________________________________________________________________________\n","1d_convnet_residual_block1 (Add (None, None, 20)     0           1d_convnet_batch_norm1b[0][0]    \n","                                                                 input_layer[0][0]                \n","__________________________________________________________________________________________________\n","1d_convnet_layer2a (Conv1D)     (None, None, 20)     6820        1d_convnet_residual_block1[0][0] \n","__________________________________________________________________________________________________\n","1d_convnet_batch_norm2a (BatchN (None, None, 20)     80          1d_convnet_layer2a[0][0]         \n","__________________________________________________________________________________________________\n","1d_convnet_layer2b (Conv1D)     (None, None, 20)     6820        1d_convnet_batch_norm2a[0][0]    \n","__________________________________________________________________________________________________\n","1d_convnet_batch_norm2b (BatchN (None, None, 20)     80          1d_convnet_layer2b[0][0]         \n","__________________________________________________________________________________________________\n","1d_convnet_residual_block2 (Add (None, None, 20)     0           1d_convnet_batch_norm2b[0][0]    \n","                                                                 1d_convnet_residual_block1[0][0] \n","__________________________________________________________________________________________________\n","1d_convnet_layer3a (Conv1D)     (None, None, 20)     6820        1d_convnet_residual_block2[0][0] \n","__________________________________________________________________________________________________\n","1d_convnet_batch_norm3a (BatchN (None, None, 20)     80          1d_convnet_layer3a[0][0]         \n","__________________________________________________________________________________________________\n","1d_convnet_layer3b (Conv1D)     (None, None, 20)     6820        1d_convnet_batch_norm3a[0][0]    \n","__________________________________________________________________________________________________\n","1d_convnet_batch_norm3b (BatchN (None, None, 20)     80          1d_convnet_layer3b[0][0]         \n","__________________________________________________________________________________________________\n","1d_convnet_residual_block3 (Add (None, None, 20)     0           1d_convnet_batch_norm3b[0][0]    \n","                                                                 1d_convnet_residual_block2[0][0] \n","__________________________________________________________________________________________________\n","outer_product (OuterProduct)    (None, None, None, 6 0           1d_convnet_residual_block3[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_layer1a (Conv2D)     (None, None, None, 6 32460       outer_product[0][0]              \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm1a (BatchN (None, None, None, 6 240         2d_convnet_layer1a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer1b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm1a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm1b (BatchN (None, None, None, 6 240         2d_convnet_layer1b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block1 (Add (None, None, None, 6 0           2d_convnet_batch_norm1b[0][0]    \n","                                                                 outer_product[0][0]              \n","__________________________________________________________________________________________________\n","2d_convnet_layer2a (Conv2D)     (None, None, None, 6 32460       2d_convnet_residual_block1[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm2a (BatchN (None, None, None, 6 240         2d_convnet_layer2a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer2b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm2a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm2b (BatchN (None, None, None, 6 240         2d_convnet_layer2b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block2 (Add (None, None, None, 6 0           2d_convnet_batch_norm2b[0][0]    \n","                                                                 2d_convnet_residual_block1[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_layer3a (Conv2D)     (None, None, None, 6 32460       2d_convnet_residual_block2[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm3a (BatchN (None, None, None, 6 240         2d_convnet_layer3a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer3b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm3a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm3b (BatchN (None, None, None, 6 240         2d_convnet_layer3b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block3 (Add (None, None, None, 6 0           2d_convnet_batch_norm3b[0][0]    \n","                                                                 2d_convnet_residual_block2[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_layer4a (Conv2D)     (None, None, None, 6 32460       2d_convnet_residual_block3[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm4a (BatchN (None, None, None, 6 240         2d_convnet_layer4a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer4b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm4a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm4b (BatchN (None, None, None, 6 240         2d_convnet_layer4b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block4 (Add (None, None, None, 6 0           2d_convnet_batch_norm4b[0][0]    \n","                                                                 2d_convnet_residual_block3[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_layer5a (Conv2D)     (None, None, None, 6 32460       2d_convnet_residual_block4[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm5a (BatchN (None, None, None, 6 240         2d_convnet_layer5a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer5b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm5a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm5b (BatchN (None, None, None, 6 240         2d_convnet_layer5b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block5 (Add (None, None, None, 6 0           2d_convnet_batch_norm5b[0][0]    \n","                                                                 2d_convnet_residual_block4[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_layer6a (Conv2D)     (None, None, None, 6 32460       2d_convnet_residual_block5[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm6a (BatchN (None, None, None, 6 240         2d_convnet_layer6a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer6b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm6a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm6b (BatchN (None, None, None, 6 240         2d_convnet_layer6b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block6 (Add (None, None, None, 6 0           2d_convnet_batch_norm6b[0][0]    \n","                                                                 2d_convnet_residual_block5[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_layer7a (Conv2D)     (None, None, None, 6 32460       2d_convnet_residual_block6[0][0] \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm7a (BatchN (None, None, None, 6 240         2d_convnet_layer7a[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_layer7b (Conv2D)     (None, None, None, 6 32460       2d_convnet_batch_norm7a[0][0]    \n","__________________________________________________________________________________________________\n","2d_convnet_batch_norm7b (BatchN (None, None, None, 6 240         2d_convnet_layer7b[0][0]         \n","__________________________________________________________________________________________________\n","2d_convnet_residual_block7 (Add (None, None, None, 6 0           2d_convnet_batch_norm7b[0][0]    \n","                                                                 2d_convnet_residual_block6[0][0] \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, None, None, 1 61          2d_convnet_residual_block7[0][0] \n","__________________________________________________________________________________________________\n","batch_normalization_v1_1 (Batch (None, None, None, 1 4           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, None)         0           batch_normalization_v1_1[0][0]   \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, None, 1)      0           flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, None, 1)      0           lambda_1[0][0]                   \n","==================================================================================================\n","Total params: 499,265\n","Trainable params: 497,343\n","Non-trainable params: 1,922\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"Lj6ZWPAYXndc","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(\n","        optimizer=\"adam\",\n","        loss=\"binary_crossentropy\",\n","        sample_weight_mode=\"temporal\",\n","        metrics=['accuracy']\n","    )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_7REYyXLXndf","colab_type":"code","colab":{}},"cell_type":"code","source":["# history = model.fit_generator(\n","#         aa_generator(train_aa_dict, train_cmap_dict),\n","#         validation_data=aa_generator(valid_aa_dict, valid_cmap_dict),\n","#         steps_per_epoch=1500, \n","#         epochs=20,\n","#         validation_steps=10,\n","#         callbacks=callbacks_list\n","#     )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ApvYJ6onXndi","colab_type":"code","outputId":"46b8396a-923f-4c7d-bc10-2d7b14f8433a","executionInfo":{"status":"ok","timestamp":1553906219302,"user_tz":240,"elapsed":431,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["batch_size = 1\n","train_steps = round (len (train_aa_dict) / batch_size)\n","valid_steps = round (len (valid_aa_dict) / batch_size)\n","print (train_steps)\n","print (valid_steps)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["746\n","213\n"],"name":"stdout"}]},{"metadata":{"id":"OpwxYQaeAbrs","colab_type":"code","colab":{}},"cell_type":"code","source":["config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ObSWZn3hPzr-","colab_type":"code","outputId":"c6f93a3e-65f5-4ff5-e931-874131ba4ffb","executionInfo":{"status":"ok","timestamp":1553910341903,"user_tz":240,"elapsed":4117995,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}},"colab":{"base_uri":"https://localhost:8080/","height":1935}},"cell_type":"code","source":["class_weight={\n","    0:1,\n","    1:8\n","}\n","history = model.fit_generator(\n","        aa_generator_batch(train_aa_dict, train_cmap_dict, batch_size),\n","        validation_data=aa_generator_batch(valid_aa_dict, valid_cmap_dict, batch_size),\n","        steps_per_epoch=train_steps, \n","        epochs=20,\n","        validation_steps=valid_steps,\n","        callbacks=callbacks_list,\n","    )"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:492: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/20\n","213/213 [==============================] - 23s 109ms/step - loss: 0.4723 - acc: 0.9646\n","\n","Epoch 00001: val_loss improved from inf to 0.47226, saving model to best_weight/Double_Resid_Network_weights.best.hdf5\n","746/746 [==============================] - 330s 442ms/step - loss: 0.6549 - acc: 0.9319 - val_loss: 0.4723 - val_acc: 0.9646\n","Epoch 2/20\n","213/213 [==============================] - 15s 72ms/step - loss: 0.4066 - acc: 0.9602\n","\n","Epoch 00002: val_loss improved from 0.47226 to 0.40660, saving model to best_weight/Double_Resid_Network_weights.best.hdf5\n","746/746 [==============================] - 201s 269ms/step - loss: 0.4372 - acc: 0.9563 - val_loss: 0.4066 - val_acc: 0.9602\n","Epoch 3/20\n","213/213 [==============================] - 14s 66ms/step - loss: 0.3933 - acc: 0.9687\n","\n","Epoch 00003: val_loss improved from 0.40660 to 0.39326, saving model to best_weight/Double_Resid_Network_weights.best.hdf5\n","746/746 [==============================] - 197s 264ms/step - loss: 0.3968 - acc: 0.9654 - val_loss: 0.3933 - val_acc: 0.9687\n","Epoch 4/20\n","213/213 [==============================] - 16s 74ms/step - loss: 0.3960 - acc: 0.9806\n","\n","Epoch 00004: val_loss did not improve from 0.39326\n","746/746 [==============================] - 200s 268ms/step - loss: 0.3809 - acc: 0.9662 - val_loss: 0.3960 - val_acc: 0.9806\n","Epoch 5/20\n","213/213 [==============================] - 14s 65ms/step - loss: 0.4496 - acc: 0.9547\n","\n","Epoch 00005: val_loss did not improve from 0.39326\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","746/746 [==============================] - 199s 267ms/step - loss: 0.3752 - acc: 0.9678 - val_loss: 0.4496 - val_acc: 0.9547\n","Epoch 6/20\n","213/213 [==============================] - 16s 74ms/step - loss: 0.3559 - acc: 0.9746\n","\n","Epoch 00006: val_loss improved from 0.39326 to 0.35587, saving model to best_weight/Double_Resid_Network_weights.best.hdf5\n","746/746 [==============================] - 198s 266ms/step - loss: 0.3444 - acc: 0.9685 - val_loss: 0.3559 - val_acc: 0.9746\n","Epoch 7/20\n","213/213 [==============================] - 16s 77ms/step - loss: 0.3368 - acc: 0.9559\n","\n","Epoch 00007: val_loss improved from 0.35587 to 0.33681, saving model to best_weight/Double_Resid_Network_weights.best.hdf5\n","746/746 [==============================] - 202s 270ms/step - loss: 0.3284 - acc: 0.9700 - val_loss: 0.3368 - val_acc: 0.9559\n","Epoch 8/20\n","213/213 [==============================] - 14s 64ms/step - loss: 0.3565 - acc: 0.9636\n","\n","Epoch 00008: val_loss did not improve from 0.33681\n","746/746 [==============================] - 195s 262ms/step - loss: 0.3187 - acc: 0.9704 - val_loss: 0.3565 - val_acc: 0.9636\n","Epoch 9/20\n","213/213 [==============================] - 15s 69ms/step - loss: 0.3476 - acc: 0.9689\n","\n","Epoch 00009: val_loss did not improve from 0.33681\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","746/746 [==============================] - 199s 267ms/step - loss: 0.3098 - acc: 0.9705 - val_loss: 0.3476 - val_acc: 0.9689\n","Epoch 10/20\n","213/213 [==============================] - 16s 74ms/step - loss: 0.3342 - acc: 0.9667\n","\n","Epoch 00010: val_loss improved from 0.33681 to 0.33420, saving model to best_weight/Double_Resid_Network_weights.best.hdf5\n","746/746 [==============================] - 200s 268ms/step - loss: 0.2927 - acc: 0.9715 - val_loss: 0.3342 - val_acc: 0.9667\n","Epoch 11/20\n","213/213 [==============================] - 14s 67ms/step - loss: 0.3477 - acc: 0.9594\n","\n","Epoch 00011: val_loss did not improve from 0.33420\n","746/746 [==============================] - 198s 265ms/step - loss: 0.2845 - acc: 0.9714 - val_loss: 0.3477 - val_acc: 0.9594\n","Epoch 12/20\n","213/213 [==============================] - 15s 68ms/step - loss: 0.3402 - acc: 0.9642\n","\n","Epoch 00012: val_loss did not improve from 0.33420\n","\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n","746/746 [==============================] - 200s 268ms/step - loss: 0.2789 - acc: 0.9713 - val_loss: 0.3402 - val_acc: 0.9642\n","Epoch 13/20\n","213/213 [==============================] - 16s 73ms/step - loss: 0.3428 - acc: 0.9602\n","\n","Epoch 00013: val_loss did not improve from 0.33420\n","746/746 [==============================] - 197s 264ms/step - loss: 0.2709 - acc: 0.9714 - val_loss: 0.3428 - val_acc: 0.9602\n","Epoch 14/20\n","213/213 [==============================] - 16s 74ms/step - loss: 0.3431 - acc: 0.9623\n","\n","Epoch 00014: val_loss did not improve from 0.33420\n","746/746 [==============================] - 200s 268ms/step - loss: 0.2685 - acc: 0.9713 - val_loss: 0.3431 - val_acc: 0.9623\n","Epoch 15/20\n","213/213 [==============================] - 14s 67ms/step - loss: 0.3450 - acc: 0.9643\n","\n","Epoch 00015: val_loss did not improve from 0.33420\n","\n","Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n","746/746 [==============================] - 197s 264ms/step - loss: 0.2666 - acc: 0.9713 - val_loss: 0.3450 - val_acc: 0.9643\n","Epoch 16/20\n","213/213 [==============================] - 15s 70ms/step - loss: 0.3447 - acc: 0.9567\n","\n","Epoch 00016: val_loss did not improve from 0.33420\n","746/746 [==============================] - 198s 266ms/step - loss: 0.2643 - acc: 0.9713 - val_loss: 0.3447 - val_acc: 0.9567\n","Epoch 17/20\n","213/213 [==============================] - 15s 69ms/step - loss: 0.3498 - acc: 0.9563\n","\n","Epoch 00017: val_loss did not improve from 0.33420\n","746/746 [==============================] - 200s 268ms/step - loss: 0.2638 - acc: 0.9713 - val_loss: 0.3498 - val_acc: 0.9563\n","Epoch 18/20\n","213/213 [==============================] - 16s 73ms/step - loss: 0.3475 - acc: 0.9608\n","\n","Epoch 00018: val_loss did not improve from 0.33420\n","\n","Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n","746/746 [==============================] - 197s 264ms/step - loss: 0.2634 - acc: 0.9713 - val_loss: 0.3475 - val_acc: 0.9608\n","Epoch 19/20\n","213/213 [==============================] - 15s 73ms/step - loss: 0.3442 - acc: 0.9589\n","\n","Epoch 00019: val_loss did not improve from 0.33420\n","746/746 [==============================] - 200s 267ms/step - loss: 0.2629 - acc: 0.9713 - val_loss: 0.3442 - val_acc: 0.9589\n","Epoch 20/20\n","213/213 [==============================] - 14s 67ms/step - loss: 0.3495 - acc: 0.9563\n","\n","Epoch 00020: val_loss did not improve from 0.33420\n","746/746 [==============================] - 196s 263ms/step - loss: 0.2628 - acc: 0.9712 - val_loss: 0.3495 - val_acc: 0.9563\n"],"name":"stdout"}]},{"metadata":{"id":"TBY8j4hrPXPY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"8f966d5c-26fb-41b1-eb7b-6cd8e71f51a7","executionInfo":{"status":"ok","timestamp":1553823746902,"user_tz":240,"elapsed":393341,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}}},"cell_type":"code","source":["# devtest_aa_dict = np.load(data_path + 'devtest_aa_dict.npy')[()]\n","# devtest_cmap_dict = np.load(data_path + 'devtest_cmap_dict.npy')[()]\n","# train_aa_dict = np.load(data_path + 'train_aa_dict.npy')[()]\n","# train_cmap_dict = np.load(data_path + 'train_cmap_dict.npy')[()]\n","# valid_aa_dict = np.load(data_path + 'valid_aa_dict.npy')[()]\n","# valid_cmap_dict = np.load(data_path + 'valid_cmap_dict.npy')[()]\n","\n","print (len (train_aa_dict))\n","print (len (valid_aa_dict))\n","print (len (devtest_aa_dict))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["746\n","213\n","106\n"],"name":"stdout"}]},{"metadata":{"id":"NPBMgx1_KTzM","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save('my_model.h5') "],"execution_count":0,"outputs":[]},{"metadata":{"id":"YAUY8PCNNkyy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"1239f7de-a22f-4ef8-bca3-51611cb7161d","executionInfo":{"status":"ok","timestamp":1553912558047,"user_tz":240,"elapsed":1484,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}}},"cell_type":"code","source":["from math import sqrt\n","test = devtest_aa_dict['1a7i']\n","prediction = model.predict(test.reshape((1,) + test.shape))\n","test_shape = int (sqrt (prediction.shape[1]))\n","prediction\n","# model.predict(test.reshape((1,) + test.shape)).shape#[0].shape"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.81449   ],\n","        [0.8251333 ],\n","        [0.77387846],\n","        ...,\n","        [0.8902965 ],\n","        [0.86979437],\n","        [0.8600838 ]]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"2UOxJcA1Mg6V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"782bc7d6-ab01-4658-d015-ef3cb7d858c7","executionInfo":{"status":"ok","timestamp":1553825149951,"user_tz":240,"elapsed":1677,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}}},"cell_type":"code","source":["!pwd"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/Protein_Structures/tertiary_structure_prediction/models/cull1\n"],"name":"stdout"}]},{"metadata":{"id":"0J2rgjDsOBUZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b0e974c4-c2e6-41e7-b298-4d46cfc436bc","executionInfo":{"status":"ok","timestamp":1553825544750,"user_tz":240,"elapsed":386,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}}},"cell_type":"code","source":["tf.__version__"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.13.1'"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"Wi2UbNRtl9fj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"429d1079-21e4-4ca9-960e-701476fe203c","executionInfo":{"status":"ok","timestamp":1553882157746,"user_tz":240,"elapsed":322,"user":{"displayName":"Jin Li","photoUrl":"","userId":"07718149360969652707"}}},"cell_type":"code","source":["test_shape"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["81"]},"metadata":{"tags":[]},"execution_count":28}]}]}